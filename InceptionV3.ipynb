{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebf9495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 10:38:23.649617: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization , MaxPooling2D,GlobalAveragePooling2D,UpSampling2D , concatenate, multiply ,Concatenate,AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import AUC, BinaryAccuracy, Precision, Recall, FalseNegatives, FalsePositives, TrueNegatives, TruePositives\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, BatchNormalization, Input, Lambda, Multiply\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import Lambda, Dense, Reshape\n",
    "from keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ff7bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14508316849998821480\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5750063104\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5085153504745099307\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 21:23:37.270105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 21:23:37.284556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 21:23:37.284700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 21:23:37.636404: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 21:23:37.636532: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 21:23:37.636624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 21:23:37.636698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /device:GPU:0 with 5483 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7575a79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 21:23:38.828453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 21:23:38.828609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 21:23:38.828711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40c04290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the GPU device\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    tf.config.experimental.set_visible_devices(physical_devices[0], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d720bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPU device\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Replace '0' with the GPU device ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b215da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcbb4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 299\n",
    "IMG_WIDTH = 299\n",
    "BATCH_SIZE = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44ff36a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753ca2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa970d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/media/maged/New Volume/NU/1-Spring 23/Grad Project 2/datasets again/Defacto + others/train\"\n",
    "test_path = \"/media/maged/New Volume/NU/1-Spring 23/Grad Project 2/datasets again/Defacto + others/test\"\n",
    "valid_path = \"/media/maged/New Volume/NU/1-Spring 23/Grad Project 2/datasets again/Defacto + others/validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5177da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33565 images belonging to 2 classes.\n",
      "Found 5753 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_batches = valid_datagen.flow_from_directory(\n",
    "    valid_path,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c02d3e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_count = train_batches.samples\n",
    "valid_image_count = valid_batches.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60848438",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = math.ceil(train_image_count / BATCH_SIZE)\n",
    "validation_steps = math.ceil(valid_image_count / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05c4d673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 21:23:43.990481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 21:23:43.990681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 21:23:43.990807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 21:23:43.990970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 21:23:43.991094: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-03 21:23:43.991182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5483 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "base_model = InceptionV3(weights= 'imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e78f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b79c343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom classification layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5db6d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5240f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c09e9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "attention = Conv2D(1, (1, 1), activation='sigmoid')(x)\n",
    "attended_features = multiply([x, attention])   \n",
    "    \n",
    "# Classification layers\n",
    "x = Flatten()(attended_features)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a68a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new classification layers on top\n",
    "x1 = base_model.output\n",
    "x1 = GlobalAveragePooling2D()(x1)\n",
    "x1 = Dense(256, activation='relu')(x1)\n",
    "\n",
    "# Add local feature branch\n",
    "x2 = Conv2D(128, (3, 3), activation='relu')(base_model.output)\n",
    "x2 = MaxPooling2D(pool_size=(2, 2))(x2)\n",
    "x2 = Flatten()(x2)\n",
    "x2 = Dense(256, activation='relu')(x2)\n",
    "\n",
    "# Concatenate global and local features\n",
    "x = concatenate([x1, x2])\n",
    "output = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf7b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new classification layers on top\n",
    "x1 = base_model.output\n",
    "x1 = GlobalAveragePooling2D()(x1)\n",
    "x1 = Dense(512, activation='relu')(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "x1 = Dense(256, activation='relu')(x1)\n",
    "x1 = BatchNormalization()(x1)\n",
    "x1 = Dropout(0.5)(x1)\n",
    "\n",
    "# Add local feature branch\n",
    "x2 = Conv2D(256, (3, 3), activation='relu')(base_model.output)\n",
    "x2 = MaxPooling2D(pool_size=(2, 2))(x2)\n",
    "x2 = Flatten()(x2)\n",
    "x2 = Dense(512, activation='relu')(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "x2 = Dense(256, activation='relu')(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Dropout(0.5)(x2)\n",
    "\n",
    "# Reshape x1 to match dimensions for attention\n",
    "x1_reshape = Reshape((1, 1, 256))(x1)\n",
    "\n",
    "# Spatial Attention\n",
    "att1 = Conv2D(1, (1, 1), activation='sigmoid')(x1_reshape)\n",
    "x1_att = Multiply()([x1, att1])\n",
    "\n",
    "# Flatten x1_att\n",
    "x1_att_flat = Flatten()(x1_att)\n",
    "\n",
    "# Channel-wise Attention\n",
    "att2 = Dense(256, activation='sigmoid')(x1_att_flat)\n",
    "att2 = Reshape((1, 1, 256))(att2)\n",
    "x2_att = Multiply()([x2, att2])\n",
    "x2_att = Flatten()(x2_att)\n",
    "\n",
    "# Concatenate global and local features\n",
    "x = concatenate([x1_att_flat, x2_att])\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03590d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 20:44:52.156055: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 10.29GiB (rounded to 11045437440)requested by op StatelessRandomUniformV2\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-02 20:44:52.156124: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2023-07-02 20:44:52.156134: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 161, Chunks in use: 161. 40.2KiB allocated for chunks. 40.2KiB in use in bin. 14.3KiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156139: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 174, Chunks in use: 174. 117.8KiB allocated for chunks. 117.8KiB in use in bin. 109.3KiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156143: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 59, Chunks in use: 59. 86.8KiB allocated for chunks. 86.8KiB in use in bin. 83.4KiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156146: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 3, Chunks in use: 3. 9.8KiB allocated for chunks. 9.8KiB in use in bin. 8.2KiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156149: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 2, Chunks in use: 1. 11.0KiB allocated for chunks. 4.0KiB in use in bin. 4.0KiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156153: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 3, Chunks in use: 2. 33.2KiB allocated for chunks. 23.8KiB in use in bin. 16.0KiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156156: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 2, Chunks in use: 2. 38.8KiB allocated for chunks. 38.8KiB in use in bin. 36.0KiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156159: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 11, Chunks in use: 9. 444.8KiB allocated for chunks. 378.8KiB in use in bin. 354.0KiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156162: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 13, Chunks in use: 12. 1018.8KiB allocated for chunks. 894.8KiB in use in bin. 790.0KiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156164: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 7, Chunks in use: 6. 1.41MiB allocated for chunks. 1.28MiB in use in bin. 1.12MiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156168: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 15, Chunks in use: 14. 5.83MiB allocated for chunks. 5.36MiB in use in bin. 4.95MiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156174: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 43, Chunks in use: 42. 32.82MiB allocated for chunks. 32.27MiB in use in bin. 27.94MiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156180: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 9, Chunks in use: 8. 14.92MiB allocated for chunks. 13.84MiB in use in bin. 11.83MiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156183: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 11, Chunks in use: 11. 30.59MiB allocated for chunks. 30.59MiB in use in bin. 23.30MiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156186: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 2, Chunks in use: 2. 10.17MiB allocated for chunks. 10.17MiB in use in bin. 9.41MiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156188: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 1, Chunks in use: 1. 8.03MiB allocated for chunks. 8.03MiB in use in bin. 5.91MiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156192: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156197: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156202: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156207: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 1. 144.50MiB allocated for chunks. 144.50MiB in use in bin. 144.50MiB client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156209: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 2, Chunks in use: 0. 5.14GiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-07-02 20:44:52.156213: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 10.29GiB was 256.00MiB, Chunk State: \n",
      "2023-07-02 20:44:52.156224: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 289.00MiB | Requested Size: 144.50MiB | in_use: 0 | bin_num: 20, prev:   Size: 2.50MiB | Requested Size: 2.50MiB | in_use: 1 | bin_num: -1, next:   Size: 144.50MiB | Requested Size: 144.50MiB | in_use: 1 | bin_num: -1\n",
      "2023-07-02 20:44:52.156230: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 4.85GiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 144.50MiB | Requested Size: 144.50MiB | in_use: 1 | bin_num: -1\n",
      "2023-07-02 20:44:52.156233: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 5777915904\n",
      "2023-07-02 20:44:52.156239: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2000000 of size 1280 next 1\n",
      "2023-07-02 20:44:52.156242: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2000500 of size 256 next 2\n",
      "2023-07-02 20:44:52.156246: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2000600 of size 256 next 3\n",
      "2023-07-02 20:44:52.156249: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2000700 of size 256 next 5\n",
      "2023-07-02 20:44:52.156252: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2000800 of size 256 next 6\n",
      "2023-07-02 20:44:52.156255: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2000900 of size 256 next 4\n",
      "2023-07-02 20:44:52.156258: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2000a00 of size 256 next 7\n",
      "2023-07-02 20:44:52.156261: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2000b00 of size 256 next 10\n",
      "2023-07-02 20:44:52.156264: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2000c00 of size 256 next 11\n",
      "2023-07-02 20:44:52.156268: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2000d00 of size 256 next 12\n",
      "2023-07-02 20:44:52.156271: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2000e00 of size 256 next 15\n",
      "2023-07-02 20:44:52.156274: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2000f00 of size 256 next 13\n",
      "2023-07-02 20:44:52.156277: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2001000 of size 256 next 14\n",
      "2023-07-02 20:44:52.156281: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2001100 of size 256 next 18\n",
      "2023-07-02 20:44:52.156284: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2001200 of size 256 next 19\n",
      "2023-07-02 20:44:52.156287: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2001300 of size 256 next 20\n",
      "2023-07-02 20:44:52.156290: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2001400 of size 256 next 21\n",
      "2023-07-02 20:44:52.156293: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2001500 of size 256 next 24\n",
      "2023-07-02 20:44:52.156296: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2001600 of size 256 next 25\n",
      "2023-07-02 20:44:52.156300: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2001700 of size 256 next 26\n",
      "2023-07-02 20:44:52.156303: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2001800 of size 512 next 29\n",
      "2023-07-02 20:44:52.156309: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2001a00 of size 512 next 27\n",
      "2023-07-02 20:44:52.156312: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2001c00 of size 256 next 49\n",
      "2023-07-02 20:44:52.156318: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2001d00 of size 256 next 28\n",
      "2023-07-02 20:44:52.156322: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2001e00 of size 256 next 30\n",
      "2023-07-02 20:44:52.156325: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2001f00 of size 256 next 32\n",
      "2023-07-02 20:44:52.156329: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2002000 of size 768 next 35\n",
      "2023-07-02 20:44:52.156332: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2002300 of size 768 next 8\n",
      "2023-07-02 20:44:52.156336: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2002600 of size 256 next 462\n",
      "2023-07-02 20:44:52.156339: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2002700 of size 512 next 17\n",
      "2023-07-02 20:44:52.156342: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2002900 of size 768 next 464\n",
      "2023-07-02 20:44:52.156346: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2002c00 of size 512 next 46\n",
      "2023-07-02 20:44:52.156349: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2002e00 of size 512 next 47\n",
      "2023-07-02 20:44:52.156352: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2003000 of size 512 next 56\n",
      "2023-07-02 20:44:52.156355: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2003200 of size 1280 next 33\n",
      "2023-07-02 20:44:52.156359: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2003700 of size 256 next 34\n",
      "2023-07-02 20:44:52.156362: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2003800 of size 256 next 38\n",
      "2023-07-02 20:44:52.156365: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2003900 of size 256 next 39\n",
      "2023-07-02 20:44:52.156369: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2003a00 of size 256 next 40\n",
      "2023-07-02 20:44:52.156372: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2003b00 of size 256 next 43\n",
      "2023-07-02 20:44:52.156375: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2003c00 of size 256 next 44\n",
      "2023-07-02 20:44:52.156379: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2003d00 of size 256 next 45\n",
      "2023-07-02 20:44:52.156382: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2003e00 of size 256 next 226\n",
      "2023-07-02 20:44:52.156385: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2003f00 of size 256 next 230\n",
      "2023-07-02 20:44:52.156389: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2004000 of size 768 next 231\n",
      "2023-07-02 20:44:52.156392: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2004300 of size 768 next 232\n",
      "2023-07-02 20:44:52.156396: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2004600 of size 768 next 235\n",
      "2023-07-02 20:44:52.156399: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2004900 of size 768 next 236\n",
      "2023-07-02 20:44:52.156402: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2004c00 of size 768 next 237\n",
      "2023-07-02 20:44:52.156405: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2004f00 of size 768 next 238\n",
      "2023-07-02 20:44:52.156408: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2005200 of size 768 next 241\n",
      "2023-07-02 20:44:52.156411: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2005500 of size 768 next 239\n",
      "2023-07-02 20:44:52.156415: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2005800 of size 768 next 240\n",
      "2023-07-02 20:44:52.156418: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2005b00 of size 768 next 245\n",
      "2023-07-02 20:44:52.156421: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2005e00 of size 768 next 243\n",
      "2023-07-02 20:44:52.156425: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2006100 of size 768 next 244\n",
      "2023-07-02 20:44:52.156429: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2006400 of size 768 next 249\n",
      "2023-07-02 20:44:52.156433: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2006700 of size 768 next 247\n",
      "2023-07-02 20:44:52.156437: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2006a00 of size 768 next 248\n",
      "2023-07-02 20:44:52.156440: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2006d00 of size 768 next 251\n",
      "2023-07-02 20:44:52.156442: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2007000 of size 768 next 252\n",
      "2023-07-02 20:44:52.156444: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2007300 of size 768 next 255\n",
      "2023-07-02 20:44:52.156446: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2007600 of size 768 next 256\n",
      "2023-07-02 20:44:52.156448: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2007900 of size 768 next 257\n",
      "2023-07-02 20:44:52.156451: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2007c00 of size 768 next 259\n",
      "2023-07-02 20:44:52.156455: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2007f00 of size 768 next 260\n",
      "2023-07-02 20:44:52.156460: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2008200 of size 768 next 261\n",
      "2023-07-02 20:44:52.156464: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2008500 of size 768 next 262\n",
      "2023-07-02 20:44:52.156468: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2008800 of size 768 next 263\n",
      "2023-07-02 20:44:52.156472: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2008b00 of size 768 next 264\n",
      "2023-07-02 20:44:52.156475: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2008e00 of size 768 next 265\n",
      "2023-07-02 20:44:52.156480: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2009100 of size 768 next 268\n",
      "2023-07-02 20:44:52.156483: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2009400 of size 768 next 266\n",
      "2023-07-02 20:44:52.156487: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2009700 of size 768 next 267\n",
      "2023-07-02 20:44:52.156491: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2009a00 of size 768 next 271\n",
      "2023-07-02 20:44:52.156495: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2009d00 of size 768 next 272\n",
      "2023-07-02 20:44:52.156499: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200a000 of size 768 next 275\n",
      "2023-07-02 20:44:52.156503: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200a300 of size 768 next 276\n",
      "2023-07-02 20:44:52.156507: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200a600 of size 768 next 277\n",
      "2023-07-02 20:44:52.156511: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200a900 of size 768 next 278\n",
      "2023-07-02 20:44:52.156514: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200ac00 of size 768 next 281\n",
      "2023-07-02 20:44:52.156518: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200af00 of size 768 next 279\n",
      "2023-07-02 20:44:52.156521: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200b200 of size 768 next 280\n",
      "2023-07-02 20:44:52.156524: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200b500 of size 768 next 285\n",
      "2023-07-02 20:44:52.156527: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200b800 of size 768 next 283\n",
      "2023-07-02 20:44:52.156530: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200bb00 of size 768 next 284\n",
      "2023-07-02 20:44:52.156536: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200be00 of size 768 next 289\n",
      "2023-07-02 20:44:52.156539: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200c100 of size 768 next 287\n",
      "2023-07-02 20:44:52.156543: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200c400 of size 768 next 288\n",
      "2023-07-02 20:44:52.156546: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200c700 of size 768 next 291\n",
      "2023-07-02 20:44:52.156550: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200ca00 of size 768 next 292\n",
      "2023-07-02 20:44:52.156553: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200cd00 of size 768 next 295\n",
      "2023-07-02 20:44:52.156558: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200d000 of size 1024 next 31\n",
      "2023-07-02 20:44:52.156562: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a200d400 of size 69632 next 16\n",
      "2023-07-02 20:44:52.156566: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a201e400 of size 37632 next 48\n",
      "2023-07-02 20:44:52.156570: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2027700 of size 256 next 50\n",
      "2023-07-02 20:44:52.156574: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2027800 of size 256 next 51\n",
      "2023-07-02 20:44:52.156577: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2027900 of size 256 next 54\n",
      "2023-07-02 20:44:52.156581: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2027a00 of size 256 next 52\n",
      "2023-07-02 20:44:52.156584: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2027b00 of size 256 next 53\n",
      "2023-07-02 20:44:52.156587: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2027c00 of size 256 next 55\n",
      "2023-07-02 20:44:52.156590: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2027d00 of size 256 next 57\n",
      "2023-07-02 20:44:52.156594: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2027e00 of size 256 next 60\n",
      "2023-07-02 20:44:52.156597: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2027f00 of size 256 next 61\n",
      "2023-07-02 20:44:52.156601: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2028000 of size 256 next 62\n",
      "2023-07-02 20:44:52.156604: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2028100 of size 512 next 65\n",
      "2023-07-02 20:44:52.156608: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2028300 of size 512 next 63\n",
      "2023-07-02 20:44:52.156612: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2028500 of size 256 next 22\n",
      "2023-07-02 20:44:52.156616: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2028600 of size 256 next 64\n",
      "2023-07-02 20:44:52.156619: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2028700 of size 256 next 67\n",
      "2023-07-02 20:44:52.156622: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2028800 of size 256 next 68\n",
      "2023-07-02 20:44:52.156626: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2028900 of size 512 next 69\n",
      "2023-07-02 20:44:52.156630: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2028b00 of size 512 next 70\n",
      "2023-07-02 20:44:52.156634: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2028d00 of size 256 next 77\n",
      "2023-07-02 20:44:52.156638: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2028e00 of size 256 next 73\n",
      "2023-07-02 20:44:52.156642: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2028f00 of size 256 next 74\n",
      "2023-07-02 20:44:52.156646: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2029000 of size 256 next 75\n",
      "2023-07-02 20:44:52.156649: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2029100 of size 512 next 164\n",
      "2023-07-02 20:44:52.156652: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2029300 of size 512 next 163\n",
      "2023-07-02 20:44:52.156655: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2029500 of size 256 next 167\n",
      "2023-07-02 20:44:52.156659: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2029600 of size 256 next 168\n",
      "2023-07-02 20:44:52.156662: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2029700 of size 768 next 169\n",
      "2023-07-02 20:44:52.156666: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2029a00 of size 768 next 170\n",
      "2023-07-02 20:44:52.156670: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2029d00 of size 768 next 173\n",
      "2023-07-02 20:44:52.156673: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202a000 of size 256 next 174\n",
      "2023-07-02 20:44:52.156677: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202a100 of size 256 next 175\n",
      "2023-07-02 20:44:52.156681: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202a200 of size 512 next 176\n",
      "2023-07-02 20:44:52.156685: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202a400 of size 512 next 177\n",
      "2023-07-02 20:44:52.156688: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202a600 of size 512 next 179\n",
      "2023-07-02 20:44:52.156691: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202a800 of size 256 next 180\n",
      "2023-07-02 20:44:52.156695: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202a900 of size 256 next 181\n",
      "2023-07-02 20:44:52.156699: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202aa00 of size 512 next 182\n",
      "2023-07-02 20:44:52.156704: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202ac00 of size 512 next 183\n",
      "2023-07-02 20:44:52.156707: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202ae00 of size 512 next 185\n",
      "2023-07-02 20:44:52.156711: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202b000 of size 256 next 186\n",
      "2023-07-02 20:44:52.156714: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202b100 of size 256 next 187\n",
      "2023-07-02 20:44:52.156718: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202b200 of size 768 next 190\n",
      "2023-07-02 20:44:52.156722: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202b500 of size 768 next 188\n",
      "2023-07-02 20:44:52.156726: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202b800 of size 768 next 189\n",
      "2023-07-02 20:44:52.156730: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202bb00 of size 512 next 193\n",
      "2023-07-02 20:44:52.156734: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202bd00 of size 512 next 194\n",
      "2023-07-02 20:44:52.156738: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202bf00 of size 512 next 196\n",
      "2023-07-02 20:44:52.156742: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202c100 of size 512 next 197\n",
      "2023-07-02 20:44:52.156747: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202c300 of size 512 next 198\n",
      "2023-07-02 20:44:52.156751: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202c500 of size 512 next 200\n",
      "2023-07-02 20:44:52.156754: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202c700 of size 512 next 201\n",
      "2023-07-02 20:44:52.156756: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202c900 of size 512 next 202\n",
      "2023-07-02 20:44:52.156758: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202cb00 of size 512 next 203\n",
      "2023-07-02 20:44:52.156760: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202cd00 of size 512 next 204\n",
      "2023-07-02 20:44:52.156762: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202cf00 of size 512 next 205\n",
      "2023-07-02 20:44:52.156764: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202d100 of size 1280 next 210\n",
      "2023-07-02 20:44:52.156768: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202d600 of size 768 next 208\n",
      "2023-07-02 20:44:52.156773: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202d900 of size 768 next 209\n",
      "2023-07-02 20:44:52.156777: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202dc00 of size 768 next 211\n",
      "2023-07-02 20:44:52.156780: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202df00 of size 768 next 213\n",
      "2023-07-02 20:44:52.156784: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202e200 of size 768 next 214\n",
      "2023-07-02 20:44:52.156787: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202e500 of size 768 next 216\n",
      "2023-07-02 20:44:52.156791: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202e800 of size 768 next 217\n",
      "2023-07-02 20:44:52.156794: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202eb00 of size 768 next 219\n",
      "2023-07-02 20:44:52.156797: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202ee00 of size 256 next 220\n",
      "2023-07-02 20:44:52.156801: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202ef00 of size 256 next 221\n",
      "2023-07-02 20:44:52.156805: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202f000 of size 768 next 222\n",
      "2023-07-02 20:44:52.156809: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202f300 of size 768 next 223\n",
      "2023-07-02 20:44:52.156813: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202f600 of size 768 next 224\n",
      "2023-07-02 20:44:52.156817: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202f900 of size 768 next 227\n",
      "2023-07-02 20:44:52.156821: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202fc00 of size 768 next 225\n",
      "2023-07-02 20:44:52.156826: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a202ff00 of size 1280 next 23\n",
      "2023-07-02 20:44:52.156829: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2030400 of size 99072 next 78\n",
      "2023-07-02 20:44:52.156833: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2048700 of size 256 next 80\n",
      "2023-07-02 20:44:52.156836: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2048800 of size 256 next 81\n",
      "2023-07-02 20:44:52.156840: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2048900 of size 256 next 84\n",
      "2023-07-02 20:44:52.156843: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2048a00 of size 256 next 82\n",
      "2023-07-02 20:44:52.156846: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2048b00 of size 256 next 83\n",
      "2023-07-02 20:44:52.156849: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2048c00 of size 256 next 87\n",
      "2023-07-02 20:44:52.156854: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2048d00 of size 256 next 88\n",
      "2023-07-02 20:44:52.156858: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2048e00 of size 256 next 89\n",
      "2023-07-02 20:44:52.156862: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2048f00 of size 256 next 90\n",
      "2023-07-02 20:44:52.156865: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2049000 of size 256 next 91\n",
      "2023-07-02 20:44:52.156869: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2049100 of size 256 next 93\n",
      "2023-07-02 20:44:52.156873: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2049200 of size 256 next 94\n",
      "2023-07-02 20:44:52.156877: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2049300 of size 256 next 97\n",
      "2023-07-02 20:44:52.156880: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2049400 of size 256 next 100\n",
      "2023-07-02 20:44:52.156884: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2049500 of size 256 next 98\n",
      "2023-07-02 20:44:52.156887: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2049600 of size 256 next 99\n",
      "2023-07-02 20:44:52.156891: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2049700 of size 512 next 102\n",
      "2023-07-02 20:44:52.156894: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2049900 of size 512 next 103\n",
      "2023-07-02 20:44:52.156898: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2049b00 of size 512 next 104\n",
      "2023-07-02 20:44:52.156901: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2049d00 of size 512 next 108\n",
      "2023-07-02 20:44:52.156904: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2049f00 of size 512 next 106\n",
      "2023-07-02 20:44:52.156908: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204a100 of size 256 next 107\n",
      "2023-07-02 20:44:52.156912: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204a200 of size 256 next 113\n",
      "2023-07-02 20:44:52.156916: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204a300 of size 256 next 71\n",
      "2023-07-02 20:44:52.156919: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204a400 of size 512 next 112\n",
      "2023-07-02 20:44:52.156923: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204a600 of size 256 next 115\n",
      "2023-07-02 20:44:52.156928: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204a700 of size 256 next 116\n",
      "2023-07-02 20:44:52.156932: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204a800 of size 256 next 117\n",
      "2023-07-02 20:44:52.156936: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204a900 of size 256 next 118\n",
      "2023-07-02 20:44:52.156939: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204aa00 of size 256 next 119\n",
      "2023-07-02 20:44:52.156943: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204ab00 of size 256 next 121\n",
      "2023-07-02 20:44:52.156946: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204ac00 of size 256 next 122\n",
      "2023-07-02 20:44:52.156949: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204ad00 of size 256 next 123\n",
      "2023-07-02 20:44:52.156953: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204ae00 of size 256 next 124\n",
      "2023-07-02 20:44:52.156956: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204af00 of size 256 next 125\n",
      "2023-07-02 20:44:52.156960: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204b000 of size 256 next 129\n",
      "2023-07-02 20:44:52.156964: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204b100 of size 256 next 127\n",
      "2023-07-02 20:44:52.156967: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204b200 of size 256 next 128\n",
      "2023-07-02 20:44:52.156971: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204b300 of size 256 next 131\n",
      "2023-07-02 20:44:52.156975: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204b400 of size 256 next 132\n",
      "2023-07-02 20:44:52.156979: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204b500 of size 256 next 135\n",
      "2023-07-02 20:44:52.156982: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204b600 of size 512 next 138\n",
      "2023-07-02 20:44:52.156986: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204b800 of size 512 next 136\n",
      "2023-07-02 20:44:52.156990: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204ba00 of size 512 next 137\n",
      "2023-07-02 20:44:52.156994: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204bc00 of size 512 next 139\n",
      "2023-07-02 20:44:52.156999: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204be00 of size 512 next 141\n",
      "2023-07-02 20:44:52.157003: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204c000 of size 256 next 144\n",
      "2023-07-02 20:44:52.157007: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204c100 of size 256 next 145\n",
      "2023-07-02 20:44:52.157012: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204c200 of size 256 next 130\n",
      "2023-07-02 20:44:52.157016: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204c300 of size 512 next 147\n",
      "2023-07-02 20:44:52.157018: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204c500 of size 256 next 148\n",
      "2023-07-02 20:44:52.157020: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204c600 of size 256 next 149\n",
      "2023-07-02 20:44:52.157022: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204c700 of size 1536 next 152\n",
      "2023-07-02 20:44:52.157024: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204cd00 of size 1536 next 150\n",
      "2023-07-02 20:44:52.157026: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204d300 of size 1536 next 151\n",
      "2023-07-02 20:44:52.157031: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204d900 of size 256 next 155\n",
      "2023-07-02 20:44:52.157036: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204da00 of size 256 next 156\n",
      "2023-07-02 20:44:52.157039: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204db00 of size 768 next 159\n",
      "2023-07-02 20:44:52.157043: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204de00 of size 512 next 160\n",
      "2023-07-02 20:44:52.157046: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204e000 of size 512 next 161\n",
      "2023-07-02 20:44:52.157049: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204e200 of size 512 next 42\n",
      "2023-07-02 20:44:52.157053: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a204e400 of size 49152 next 41\n",
      "2023-07-02 20:44:52.157056: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205a400 of size 768 next 296\n",
      "2023-07-02 20:44:52.157060: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205a700 of size 768 next 297\n",
      "2023-07-02 20:44:52.157066: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205aa00 of size 768 next 299\n",
      "2023-07-02 20:44:52.157070: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205ad00 of size 768 next 300\n",
      "2023-07-02 20:44:52.157074: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205b000 of size 768 next 301\n",
      "2023-07-02 20:44:52.157079: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205b300 of size 768 next 302\n",
      "2023-07-02 20:44:52.157083: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205b600 of size 768 next 303\n",
      "2023-07-02 20:44:52.157086: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205b900 of size 768 next 305\n",
      "2023-07-02 20:44:52.157090: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205bc00 of size 256 next 306\n",
      "2023-07-02 20:44:52.157093: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205bd00 of size 256 next 307\n",
      "2023-07-02 20:44:52.157097: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205be00 of size 768 next 310\n",
      "2023-07-02 20:44:52.157100: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205c100 of size 768 next 308\n",
      "2023-07-02 20:44:52.157104: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205c400 of size 768 next 309\n",
      "2023-07-02 20:44:52.157113: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205c700 of size 768 next 315\n",
      "2023-07-02 20:44:52.157117: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205ca00 of size 768 next 313\n",
      "2023-07-02 20:44:52.157120: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205cd00 of size 768 next 314\n",
      "2023-07-02 20:44:52.157124: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205d000 of size 768 next 317\n",
      "2023-07-02 20:44:52.157127: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205d300 of size 768 next 318\n",
      "2023-07-02 20:44:52.157130: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205d600 of size 768 next 320\n",
      "2023-07-02 20:44:52.157134: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205d900 of size 768 next 321\n",
      "2023-07-02 20:44:52.157138: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205dc00 of size 768 next 322\n",
      "2023-07-02 20:44:52.157141: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205df00 of size 768 next 325\n",
      "2023-07-02 20:44:52.157144: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205e200 of size 768 next 326\n",
      "2023-07-02 20:44:52.157148: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205e500 of size 768 next 327\n",
      "2023-07-02 20:44:52.157152: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205e800 of size 768 next 329\n",
      "2023-07-02 20:44:52.157156: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205eb00 of size 768 next 330\n",
      "2023-07-02 20:44:52.157160: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205ee00 of size 768 next 331\n",
      "2023-07-02 20:44:52.157164: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205f100 of size 768 next 333\n",
      "2023-07-02 20:44:52.157169: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205f400 of size 768 next 334\n",
      "2023-07-02 20:44:52.157172: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205f700 of size 768 next 335\n",
      "2023-07-02 20:44:52.157176: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205fa00 of size 768 next 337\n",
      "2023-07-02 20:44:52.157179: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a205fd00 of size 768 next 338\n",
      "2023-07-02 20:44:52.157182: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2060000 of size 768 next 339\n",
      "2023-07-02 20:44:52.157186: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2060300 of size 768 next 340\n",
      "2023-07-02 20:44:52.157190: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2060600 of size 768 next 341\n",
      "2023-07-02 20:44:52.157194: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2060900 of size 768 next 342\n",
      "2023-07-02 20:44:52.157197: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2060c00 of size 768 next 344\n",
      "2023-07-02 20:44:52.157200: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2060f00 of size 256 next 345\n",
      "2023-07-02 20:44:52.157205: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2061000 of size 256 next 346\n",
      "2023-07-02 20:44:52.157208: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2061100 of size 1280 next 349\n",
      "2023-07-02 20:44:52.157212: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2061600 of size 1280 next 347\n",
      "2023-07-02 20:44:52.157216: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2061b00 of size 1280 next 348\n",
      "2023-07-02 20:44:52.157221: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2062000 of size 768 next 352\n",
      "2023-07-02 20:44:52.157225: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2062300 of size 768 next 353\n",
      "2023-07-02 20:44:52.157230: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2062600 of size 768 next 354\n",
      "2023-07-02 20:44:52.157234: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2062900 of size 768 next 357\n",
      "2023-07-02 20:44:52.157239: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2062c00 of size 768 next 355\n",
      "2023-07-02 20:44:52.157242: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2062f00 of size 768 next 356\n",
      "2023-07-02 20:44:52.157247: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2063200 of size 768 next 360\n",
      "2023-07-02 20:44:52.157249: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2063500 of size 768 next 361\n",
      "2023-07-02 20:44:52.157251: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2063800 of size 768 next 362\n",
      "2023-07-02 20:44:52.157253: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2063b00 of size 256 next 364\n",
      "2023-07-02 20:44:52.157255: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2063c00 of size 256 next 365\n",
      "2023-07-02 20:44:52.157256: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2063d00 of size 768 next 366\n",
      "2023-07-02 20:44:52.157258: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2064000 of size 768 next 367\n",
      "2023-07-02 20:44:52.157260: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2064300 of size 768 next 370\n",
      "2023-07-02 20:44:52.157262: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2064600 of size 256 next 371\n",
      "2023-07-02 20:44:52.157264: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2064700 of size 256 next 372\n",
      "2023-07-02 20:44:52.157266: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2064800 of size 1280 next 375\n",
      "2023-07-02 20:44:52.157268: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2064d00 of size 1280 next 373\n",
      "2023-07-02 20:44:52.157270: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2065200 of size 1280 next 374\n",
      "2023-07-02 20:44:52.157272: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2065700 of size 256 next 378\n",
      "2023-07-02 20:44:52.157273: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2065800 of size 256 next 379\n",
      "2023-07-02 20:44:52.157276: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2065900 of size 2816 next 59\n",
      "2023-07-02 20:44:52.157278: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2066400 of size 49152 next 58\n",
      "2023-07-02 20:44:52.157279: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2072400 of size 65536 next 114\n",
      "2023-07-02 20:44:52.157282: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2082400 of size 36864 next 463\n",
      "2023-07-02 20:44:52.157284: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a208b400 of size 36864 next 120\n",
      "2023-07-02 20:44:52.157286: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2094400 of size 688128 next 178\n",
      "2023-07-02 20:44:52.157288: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a213c400 of size 856064 next 66\n",
      "2023-07-02 20:44:52.157289: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a220d400 of size 1536 next 380\n",
      "2023-07-02 20:44:52.157291: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a220da00 of size 1536 next 381\n",
      "2023-07-02 20:44:52.157293: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a220e000 of size 256 next 384\n",
      "2023-07-02 20:44:52.157295: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a220e100 of size 256 next 385\n",
      "2023-07-02 20:44:52.157297: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a220e200 of size 1536 next 386\n",
      "2023-07-02 20:44:52.157299: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a220e800 of size 1536 next 387\n",
      "2023-07-02 20:44:52.157301: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a220ee00 of size 1536 next 389\n",
      "2023-07-02 20:44:52.157303: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a220f400 of size 1536 next 390\n",
      "2023-07-02 20:44:52.157305: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a220fa00 of size 1536 next 391\n",
      "2023-07-02 20:44:52.157307: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2210000 of size 1536 next 393\n",
      "2023-07-02 20:44:52.157309: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2210600 of size 1792 next 394\n",
      "2023-07-02 20:44:52.157311: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2210d00 of size 1792 next 395\n",
      "2023-07-02 20:44:52.157313: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2211400 of size 1792 next 398\n",
      "2023-07-02 20:44:52.157315: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2211b00 of size 256 next 399\n",
      "2023-07-02 20:44:52.157317: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2211c00 of size 256 next 400\n",
      "2023-07-02 20:44:52.157319: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2211d00 of size 1536 next 403\n",
      "2023-07-02 20:44:52.157320: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2212300 of size 1536 next 401\n",
      "2023-07-02 20:44:52.157322: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2212900 of size 1536 next 402\n",
      "2023-07-02 20:44:52.157324: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2212f00 of size 1536 next 406\n",
      "2023-07-02 20:44:52.157326: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2213500 of size 1536 next 407\n",
      "2023-07-02 20:44:52.157328: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2213b00 of size 1536 next 408\n",
      "2023-07-02 20:44:52.157330: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2214100 of size 1536 next 409\n",
      "2023-07-02 20:44:52.157332: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2214700 of size 1536 next 410\n",
      "2023-07-02 20:44:52.157334: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2214d00 of size 1536 next 412\n",
      "2023-07-02 20:44:52.157335: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2215300 of size 256 next 413\n",
      "2023-07-02 20:44:52.157337: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2215400 of size 256 next 414\n",
      "2023-07-02 20:44:52.157339: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2215500 of size 768 next 415\n",
      "2023-07-02 20:44:52.157341: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2215800 of size 768 next 416\n",
      "2023-07-02 20:44:52.157343: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2215b00 of size 768 next 417\n",
      "2023-07-02 20:44:52.157345: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2215e00 of size 256 next 418\n",
      "2023-07-02 20:44:52.157347: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2215f00 of size 256 next 419\n",
      "2023-07-02 20:44:52.157349: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2216000 of size 1280 next 422\n",
      "2023-07-02 20:44:52.157351: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2216500 of size 1280 next 420\n",
      "2023-07-02 20:44:52.157353: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2216a00 of size 1280 next 421\n",
      "2023-07-02 20:44:52.157354: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2216f00 of size 256 next 425\n",
      "2023-07-02 20:44:52.157356: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2217000 of size 256 next 426\n",
      "2023-07-02 20:44:52.157358: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2217100 of size 1536 next 427\n",
      "2023-07-02 20:44:52.157360: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2217700 of size 1536 next 428\n",
      "2023-07-02 20:44:52.157362: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2217d00 of size 1536 next 431\n",
      "2023-07-02 20:44:52.157364: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2218300 of size 1536 next 432\n",
      "2023-07-02 20:44:52.157366: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2218900 of size 768 next 456\n",
      "2023-07-02 20:44:52.157368: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2218c00 of size 256 next 457\n",
      "2023-07-02 20:44:52.157370: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2218d00 of size 256 next 191\n",
      "2023-07-02 20:44:52.157371: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2218e00 of size 512 next 455\n",
      "2023-07-02 20:44:52.157373: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2219000 of size 256 next 154\n",
      "2023-07-02 20:44:52.157375: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2219100 of size 256 next 36\n",
      "2023-07-02 20:44:52.157377: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2219200 of size 256 next 465\n",
      "2023-07-02 20:44:52.157379: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2219300 of size 256 next 92\n",
      "2023-07-02 20:44:52.157381: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2219400 of size 81920 next 86\n",
      "2023-07-02 20:44:52.157383: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a222d400 of size 65536 next 85\n",
      "2023-07-02 20:44:52.157385: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a223d400 of size 1536 next 433\n",
      "2023-07-02 20:44:52.157387: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a223da00 of size 1536 next 434\n",
      "2023-07-02 20:44:52.157389: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a223e000 of size 1536 next 435\n",
      "2023-07-02 20:44:52.157391: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a223e600 of size 1536 next 436\n",
      "2023-07-02 20:44:52.157392: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a223ec00 of size 256 next 437\n",
      "2023-07-02 20:44:52.157394: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a223ed00 of size 256 next 438\n",
      "2023-07-02 20:44:52.157396: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a223ee00 of size 1792 next 439\n",
      "2023-07-02 20:44:52.157398: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a223f500 of size 1792 next 440\n",
      "2023-07-02 20:44:52.157400: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a223fc00 of size 1536 next 443\n",
      "2023-07-02 20:44:52.157402: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2240200 of size 1536 next 446\n",
      "2023-07-02 20:44:52.157404: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2240800 of size 1536 next 448\n",
      "2023-07-02 20:44:52.157406: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2240e00 of size 1792 next 445\n",
      "2023-07-02 20:44:52.157408: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2241500 of size 1536 next 449\n",
      "2023-07-02 20:44:52.157409: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2241b00 of size 1536 next 450\n",
      "2023-07-02 20:44:52.157411: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2242100 of size 1536 next 452\n",
      "2023-07-02 20:44:52.157413: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2242700 of size 1536 next 453\n",
      "2023-07-02 20:44:52.157415: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2242d00 of size 1536 next 454\n",
      "2023-07-02 20:44:52.157417: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2243300 of size 512 next 485\n",
      "2023-07-02 20:44:52.157419: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2243500 of size 512 next 486\n",
      "2023-07-02 20:44:52.157421: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2243700 of size 256 next 489\n",
      "2023-07-02 20:44:52.157423: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2243800 of size 256 next 490\n",
      "2023-07-02 20:44:52.157425: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2243900 of size 768 next 491\n",
      "2023-07-02 20:44:52.157426: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2243c00 of size 256 next 492\n",
      "2023-07-02 20:44:52.157428: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2243d00 of size 256 next 493\n",
      "2023-07-02 20:44:52.157430: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2243e00 of size 256 next 495\n",
      "2023-07-02 20:44:52.157432: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2243f00 of size 256 next 496\n",
      "2023-07-02 20:44:52.157434: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2244000 of size 256 next 497\n",
      "2023-07-02 20:44:52.157436: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2244100 of size 256 next 459\n",
      "2023-07-02 20:44:52.157438: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2244200 of size 256 next 460\n",
      "2023-07-02 20:44:52.157440: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2244300 of size 3584 next 461\n",
      "2023-07-02 20:44:52.157442: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2245100 of size 23296 next 126\n",
      "2023-07-02 20:44:52.157444: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a224ac00 of size 79872 next 72\n",
      "2023-07-02 20:44:52.157447: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a225e400 of size 397312 next 101\n",
      "2023-07-02 20:44:52.157449: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a22bf400 of size 73728 next 134\n",
      "2023-07-02 20:44:52.157451: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a22d1400 of size 73728 next 133\n",
      "2023-07-02 20:44:52.157453: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a22e3400 of size 94208 next 96\n",
      "2023-07-02 20:44:52.157455: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a22fa400 of size 73728 next 111\n",
      "2023-07-02 20:44:52.157457: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a230c400 of size 233472 next 95\n",
      "2023-07-02 20:44:52.157459: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2345400 of size 221184 next 162\n",
      "2023-07-02 20:44:52.157461: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237b400 of size 1536 next 146\n",
      "2023-07-02 20:44:52.157463: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237ba00 of size 1792 next 215\n",
      "2023-07-02 20:44:52.157465: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237c100 of size 1792 next 377\n",
      "2023-07-02 20:44:52.157467: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237c800 of size 512 next 9\n",
      "2023-07-02 20:44:52.157469: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237ca00 of size 256 next 468\n",
      "2023-07-02 20:44:52.157471: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237cb00 of size 512 next 470\n",
      "2023-07-02 20:44:52.157473: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237cd00 of size 256 next 474\n",
      "2023-07-02 20:44:52.157475: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237ce00 of size 256 next 471\n",
      "2023-07-02 20:44:52.157476: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237cf00 of size 256 next 473\n",
      "2023-07-02 20:44:52.157478: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237d000 of size 256 next 477\n",
      "2023-07-02 20:44:52.157481: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237d100 of size 256 next 478\n",
      "2023-07-02 20:44:52.157482: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237d200 of size 256 next 481\n",
      "2023-07-02 20:44:52.157484: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237d300 of size 256 next 482\n",
      "2023-07-02 20:44:52.157486: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237d400 of size 256 next 483\n",
      "2023-07-02 20:44:52.157488: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237d500 of size 256 next 212\n",
      "2023-07-02 20:44:52.157490: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a237d600 of size 3584 next 447\n",
      "2023-07-02 20:44:52.157492: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f91a237e400 of size 32768 next 498\n",
      "2023-07-02 20:44:52.157494: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2386400 of size 32768 next 467\n",
      "2023-07-02 20:44:52.157496: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a238e400 of size 32768 next 466\n",
      "2023-07-02 20:44:52.157498: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2396400 of size 512 next 500\n",
      "2023-07-02 20:44:52.157500: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2396600 of size 256 next 502\n",
      "2023-07-02 20:44:52.157502: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2396700 of size 256 next 499\n",
      "2023-07-02 20:44:52.157503: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2396800 of size 512 next 506\n",
      "2023-07-02 20:44:52.157505: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2396a00 of size 256 next 504\n",
      "2023-07-02 20:44:52.157507: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2396b00 of size 256 next 505\n",
      "2023-07-02 20:44:52.157509: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2396c00 of size 4096 next 510\n",
      "2023-07-02 20:44:52.157511: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2397c00 of size 256 next 508\n",
      "2023-07-02 20:44:52.157513: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2397d00 of size 256 next 509\n",
      "2023-07-02 20:44:52.157515: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f91a2397e00 of size 9728 next 476\n",
      "2023-07-02 20:44:52.157517: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a239a400 of size 8192 next 475\n",
      "2023-07-02 20:44:52.157519: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a239c400 of size 256 next 514\n",
      "2023-07-02 20:44:52.157521: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a239c500 of size 256 next 513\n",
      "2023-07-02 20:44:52.157523: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a239c600 of size 256 next 515\n",
      "2023-07-02 20:44:52.157525: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a239c700 of size 256 next 517\n",
      "2023-07-02 20:44:52.157527: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a239c800 of size 256 next 518\n",
      "2023-07-02 20:44:52.157529: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f91a239c900 of size 7168 next 516\n",
      "2023-07-02 20:44:52.157531: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a239e500 of size 16128 next 469\n",
      "2023-07-02 20:44:52.157533: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a23a2400 of size 61440 next 140\n",
      "2023-07-02 20:44:52.157535: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a23b1400 of size 552960 next 109\n",
      "2023-07-02 20:44:52.157537: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2438400 of size 380928 next 157\n",
      "2023-07-02 20:44:52.157539: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2495400 of size 258048 next 143\n",
      "2023-07-02 20:44:52.157541: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a24d4400 of size 331776 next 142\n",
      "2023-07-02 20:44:52.157543: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2525400 of size 995328 next 165\n",
      "2023-07-02 20:44:52.157545: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2618400 of size 589824 next 172\n",
      "2023-07-02 20:44:52.157547: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a26a8400 of size 589824 next 171\n",
      "2023-07-02 20:44:52.157549: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2738400 of size 851968 next 184\n",
      "2023-07-02 20:44:52.157551: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2808400 of size 393216 next 195\n",
      "2023-07-02 20:44:52.157553: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2868400 of size 458752 next 199\n",
      "2023-07-02 20:44:52.157554: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a28d8400 of size 524288 next 192\n",
      "2023-07-02 20:44:52.157557: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2958400 of size 491520 next 207\n",
      "2023-07-02 20:44:52.157559: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a29d0400 of size 655360 next 206\n",
      "2023-07-02 20:44:52.157561: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a2a70400 of size 8417280 next 229\n",
      "2023-07-02 20:44:52.157563: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a3277400 of size 716800 next 228\n",
      "2023-07-02 20:44:52.157564: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a3326400 of size 860160 next 234\n",
      "2023-07-02 20:44:52.157566: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a33f8400 of size 860160 next 233\n",
      "2023-07-02 20:44:52.157569: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a34ca400 of size 716800 next 242\n",
      "2023-07-02 20:44:52.157570: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a3579400 of size 716800 next 246\n",
      "2023-07-02 20:44:52.157572: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a3628400 of size 716800 next 250\n",
      "2023-07-02 20:44:52.157574: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a36d7400 of size 860160 next 254\n",
      "2023-07-02 20:44:52.157576: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a37a9400 of size 860160 next 253\n",
      "2023-07-02 20:44:52.157578: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a387b400 of size 589824 next 298\n",
      "2023-07-02 20:44:52.157580: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a390b400 of size 331776 next 37\n",
      "2023-07-02 20:44:52.157582: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a395c400 of size 512000 next 270\n",
      "2023-07-02 20:44:52.157584: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a39d9400 of size 716800 next 269\n",
      "2023-07-02 20:44:52.157586: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a3a88400 of size 860160 next 274\n",
      "2023-07-02 20:44:52.157587: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a3b5a400 of size 860160 next 273\n",
      "2023-07-02 20:44:52.157589: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a3c2c400 of size 716800 next 282\n",
      "2023-07-02 20:44:52.157591: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a3cdb400 of size 716800 next 286\n",
      "2023-07-02 20:44:52.157593: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a3d8a400 of size 716800 next 290\n",
      "2023-07-02 20:44:52.157595: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a3e39400 of size 860160 next 294\n",
      "2023-07-02 20:44:52.157597: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a3f0b400 of size 860160 next 293\n",
      "2023-07-02 20:44:52.157599: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a3fdd400 of size 589824 next 304\n",
      "2023-07-02 20:44:52.157601: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a406d400 of size 589824 next 319\n",
      "2023-07-02 20:44:52.157602: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a40fd400 of size 73728 next 166\n",
      "2023-07-02 20:44:52.157604: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f91a410f400 of size 131072 next 488\n",
      "2023-07-02 20:44:52.157606: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a412f400 of size 131072 next 487\n",
      "2023-07-02 20:44:52.157608: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a414f400 of size 253952 next 343\n",
      "2023-07-02 20:44:52.157610: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a418d400 of size 884736 next 312\n",
      "2023-07-02 20:44:52.157612: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a4265400 of size 1032192 next 311\n",
      "2023-07-02 20:44:52.157614: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a4361400 of size 1032192 next 316\n",
      "2023-07-02 20:44:52.157616: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a445d400 of size 1032192 next 324\n",
      "2023-07-02 20:44:52.157618: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a4559400 of size 1032192 next 323\n",
      "2023-07-02 20:44:52.157620: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a4655400 of size 1032192 next 328\n",
      "2023-07-02 20:44:52.157622: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a4751400 of size 1032192 next 332\n",
      "2023-07-02 20:44:52.157624: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a484d400 of size 1032192 next 336\n",
      "2023-07-02 20:44:52.157625: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a4949400 of size 331776 next 105\n",
      "2023-07-02 20:44:52.157627: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a499a400 of size 700416 next 363\n",
      "2023-07-02 20:44:52.157629: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a4a45400 of size 1032192 next 359\n",
      "2023-07-02 20:44:52.157631: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a4b41400 of size 1032192 next 358\n",
      "2023-07-02 20:44:52.157633: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a4c3d400 of size 458752 next 110\n",
      "2023-07-02 20:44:52.157635: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a4cad400 of size 868352 next 351\n",
      "2023-07-02 20:44:52.157637: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a4d81400 of size 2211840 next 350\n",
      "2023-07-02 20:44:52.157639: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a4f9d400 of size 16384 next 484\n",
      "2023-07-02 20:44:52.157641: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f91a4fa1400 of size 34816 next 480\n",
      "2023-07-02 20:44:52.157643: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a4fa9c00 of size 51200 next 479\n",
      "2023-07-02 20:44:52.157645: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a4fb6400 of size 65536 next 503\n",
      "2023-07-02 20:44:52.157647: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f91a4fc6400 of size 126976 next 79\n",
      "2023-07-02 20:44:52.157649: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a4fe5400 of size 294912 next 76\n",
      "2023-07-02 20:44:52.157651: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a502d400 of size 245760 next 507\n",
      "2023-07-02 20:44:52.157653: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f91a5069400 of size 491520 next 369\n",
      "2023-07-02 20:44:52.157655: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a50e1400 of size 1769472 next 368\n",
      "2023-07-02 20:44:52.157657: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a5291400 of size 4472832 next 376\n",
      "2023-07-02 20:44:52.157659: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a56d5400 of size 1966080 next 383\n",
      "2023-07-02 20:44:52.157661: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a58b5400 of size 1966080 next 382\n",
      "2023-07-02 20:44:52.157663: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a5a95400 of size 1769472 next 388\n",
      "2023-07-02 20:44:52.157665: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a5c45400 of size 1769472 next 392\n",
      "2023-07-02 20:44:52.157666: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a5df5400 of size 2293760 next 397\n",
      "2023-07-02 20:44:52.157668: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a6025400 of size 2293760 next 396\n",
      "2023-07-02 20:44:52.157670: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a6255400 of size 1769472 next 411\n",
      "2023-07-02 20:44:52.157672: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a6405400 of size 442368 next 472\n",
      "2023-07-02 20:44:52.157674: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f91a6471400 of size 1130496 next 458\n",
      "2023-07-02 20:44:52.157676: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a6585400 of size 3670016 next 424\n",
      "2023-07-02 20:44:52.157678: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a6905400 of size 307200 next 501\n",
      "2023-07-02 20:44:52.157680: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f91a6950400 of size 577536 next 494\n",
      "2023-07-02 20:44:52.157682: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a69dd400 of size 1736704 next 423\n",
      "2023-07-02 20:44:52.157684: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a6b85400 of size 2752512 next 405\n",
      "2023-07-02 20:44:52.157686: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a6e25400 of size 6193152 next 404\n",
      "2023-07-02 20:44:52.157688: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a740d400 of size 3145728 next 430\n",
      "2023-07-02 20:44:52.157690: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a770d400 of size 3145728 next 429\n",
      "2023-07-02 20:44:52.157692: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a7a0d400 of size 1769472 next 451\n",
      "2023-07-02 20:44:52.157694: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a7bbd400 of size 860160 next 153\n",
      "2023-07-02 20:44:52.157696: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a7c8f400 of size 491520 next 218\n",
      "2023-07-02 20:44:52.157698: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a7d07400 of size 548864 next 442\n",
      "2023-07-02 20:44:52.157699: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a7d8d400 of size 3670016 next 441\n",
      "2023-07-02 20:44:52.157701: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a810d400 of size 3981312 next 158\n",
      "2023-07-02 20:44:52.157703: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a84d9400 of size 2293760 next 258\n",
      "2023-07-02 20:44:52.157705: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91a8709400 of size 2621440 next 444\n",
      "2023-07-02 20:44:52.157707: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f91a8989400 of size 303038464 next 512\n",
      "2023-07-02 20:44:52.157709: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f91baa89400 of size 151519232 next 511\n",
      "2023-07-02 20:44:52.157711: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f91c3b09400 of size 5212695552 next 18446744073709551615\n",
      "2023-07-02 20:44:52.157713: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2023-07-02 20:44:52.157717: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 161 Chunks of size 256 totalling 40.2KiB\n",
      "2023-07-02 20:44:52.157720: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 51 Chunks of size 512 totalling 25.5KiB\n",
      "2023-07-02 20:44:52.157722: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 123 Chunks of size 768 totalling 92.2KiB\n",
      "2023-07-02 20:44:52.157724: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1024 totalling 1.0KiB\n",
      "2023-07-02 20:44:52.157727: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 13 Chunks of size 1280 totalling 16.2KiB\n",
      "2023-07-02 20:44:52.157729: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 37 Chunks of size 1536 totalling 55.5KiB\n",
      "2023-07-02 20:44:52.157731: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 8 Chunks of size 1792 totalling 14.0KiB\n",
      "2023-07-02 20:44:52.157733: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 2816 totalling 2.8KiB\n",
      "2023-07-02 20:44:52.157735: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 3584 totalling 7.0KiB\n",
      "2023-07-02 20:44:52.157737: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 4096 totalling 4.0KiB\n",
      "2023-07-02 20:44:52.157739: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 8192 totalling 8.0KiB\n",
      "2023-07-02 20:44:52.157742: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 16128 totalling 15.8KiB\n",
      "2023-07-02 20:44:52.157744: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 16384 totalling 16.0KiB\n",
      "2023-07-02 20:44:52.157746: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 23296 totalling 22.8KiB\n",
      "2023-07-02 20:44:52.157748: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 32768 totalling 64.0KiB\n",
      "2023-07-02 20:44:52.157750: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 36864 totalling 72.0KiB\n",
      "2023-07-02 20:44:52.157752: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 37632 totalling 36.8KiB\n",
      "2023-07-02 20:44:52.157755: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 49152 totalling 96.0KiB\n",
      "2023-07-02 20:44:52.157757: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 51200 totalling 50.0KiB\n",
      "2023-07-02 20:44:52.157759: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 61440 totalling 60.0KiB\n",
      "2023-07-02 20:44:52.157761: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 65536 totalling 192.0KiB\n",
      "2023-07-02 20:44:52.157763: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 69632 totalling 68.0KiB\n",
      "2023-07-02 20:44:52.157765: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 4 Chunks of size 73728 totalling 288.0KiB\n",
      "2023-07-02 20:44:52.157768: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 79872 totalling 78.0KiB\n",
      "2023-07-02 20:44:52.157770: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 81920 totalling 80.0KiB\n",
      "2023-07-02 20:44:52.157772: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 94208 totalling 92.0KiB\n",
      "2023-07-02 20:44:52.157774: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 99072 totalling 96.8KiB\n",
      "2023-07-02 20:44:52.157776: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 131072 totalling 128.0KiB\n",
      "2023-07-02 20:44:52.157778: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 221184 totalling 216.0KiB\n",
      "2023-07-02 20:44:52.157781: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 233472 totalling 228.0KiB\n",
      "2023-07-02 20:44:52.157783: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 245760 totalling 240.0KiB\n",
      "2023-07-02 20:44:52.157785: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 253952 totalling 248.0KiB\n",
      "2023-07-02 20:44:52.157787: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 258048 totalling 252.0KiB\n",
      "2023-07-02 20:44:52.157789: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 294912 totalling 288.0KiB\n",
      "2023-07-02 20:44:52.157792: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 307200 totalling 300.0KiB\n",
      "2023-07-02 20:44:52.157794: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 331776 totalling 972.0KiB\n",
      "2023-07-02 20:44:52.157796: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 380928 totalling 372.0KiB\n",
      "2023-07-02 20:44:52.157798: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 393216 totalling 384.0KiB\n",
      "2023-07-02 20:44:52.157800: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 397312 totalling 388.0KiB\n",
      "2023-07-02 20:44:52.157802: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 442368 totalling 432.0KiB\n",
      "2023-07-02 20:44:52.157805: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 458752 totalling 896.0KiB\n",
      "2023-07-02 20:44:52.157807: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 491520 totalling 960.0KiB\n",
      "2023-07-02 20:44:52.157809: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 512000 totalling 500.0KiB\n",
      "2023-07-02 20:44:52.157811: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 524288 totalling 512.0KiB\n",
      "2023-07-02 20:44:52.157813: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 548864 totalling 536.0KiB\n",
      "2023-07-02 20:44:52.157816: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 552960 totalling 540.0KiB\n",
      "2023-07-02 20:44:52.157818: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 5 Chunks of size 589824 totalling 2.81MiB\n",
      "2023-07-02 20:44:52.157820: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 655360 totalling 640.0KiB\n",
      "2023-07-02 20:44:52.157822: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 688128 totalling 672.0KiB\n",
      "2023-07-02 20:44:52.157824: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 700416 totalling 684.0KiB\n",
      "2023-07-02 20:44:52.157827: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 8 Chunks of size 716800 totalling 5.47MiB\n",
      "2023-07-02 20:44:52.157829: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 851968 totalling 832.0KiB\n",
      "2023-07-02 20:44:52.157831: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 856064 totalling 836.0KiB\n",
      "2023-07-02 20:44:52.157833: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 9 Chunks of size 860160 totalling 7.38MiB\n",
      "2023-07-02 20:44:52.157835: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 868352 totalling 848.0KiB\n",
      "2023-07-02 20:44:52.157837: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 884736 totalling 864.0KiB\n",
      "2023-07-02 20:44:52.157839: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 995328 totalling 972.0KiB\n",
      "2023-07-02 20:44:52.157842: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 9 Chunks of size 1032192 totalling 8.86MiB\n",
      "2023-07-02 20:44:52.157844: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1736704 totalling 1.66MiB\n",
      "2023-07-02 20:44:52.157846: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 5 Chunks of size 1769472 totalling 8.44MiB\n",
      "2023-07-02 20:44:52.157848: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 1966080 totalling 3.75MiB\n",
      "2023-07-02 20:44:52.157850: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 2211840 totalling 2.11MiB\n",
      "2023-07-02 20:44:52.157852: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 2293760 totalling 6.56MiB\n",
      "2023-07-02 20:44:52.157854: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 2621440 totalling 2.50MiB\n",
      "2023-07-02 20:44:52.157856: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 2752512 totalling 2.62MiB\n",
      "2023-07-02 20:44:52.157858: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 3145728 totalling 6.00MiB\n",
      "2023-07-02 20:44:52.157861: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 3670016 totalling 7.00MiB\n",
      "2023-07-02 20:44:52.157863: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 3981312 totalling 3.80MiB\n",
      "2023-07-02 20:44:52.157865: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 4472832 totalling 4.27MiB\n",
      "2023-07-02 20:44:52.157867: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 6193152 totalling 5.91MiB\n",
      "2023-07-02 20:44:52.157869: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 8417280 totalling 8.03MiB\n",
      "2023-07-02 20:44:52.157871: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 151519232 totalling 144.50MiB\n",
      "2023-07-02 20:44:52.157873: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 247.61MiB\n",
      "2023-07-02 20:44:52.157876: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 5777915904 memory_limit_: 5777915904 available bytes: 0 curr_region_allocation_bytes_: 11555831808\n",
      "2023-07-02 20:44:52.157881: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                      5777915904\n",
      "InUse:                       259639808\n",
      "MaxInUse:                    562656768\n",
      "NumAllocs:                        1444\n",
      "MaxAllocSize:                151519232\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-07-02 20:44:52.157890: W tensorflow/tsl/framework/bfc_allocator.cc:497] **_____***__________________________________________________________________________________________\n",
      "2023-07-02 20:44:52.157908: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at stateless_random_ops_v2.cc:64 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[1348320,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[1348320,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39m# Fully Connected Layers\u001b[39;00m\n\u001b[1;32m     54\u001b[0m flatten \u001b[39m=\u001b[39m Flatten()(mixed1)  \u001b[39m# Replace mixedX with the appropriate layer\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m fc1 \u001b[39m=\u001b[39m Dense(\u001b[39m2048\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m)(flatten)\n\u001b[1;32m     56\u001b[0m dropout \u001b[39m=\u001b[39m Dropout(\u001b[39m0.5\u001b[39m)(fc1)\n\u001b[1;32m     57\u001b[0m output \u001b[39m=\u001b[39m Dense(num_classes, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)(dropout)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/backend.py:2101\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[0;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[1;32m   2099\u001b[0m     \u001b[39mif\u001b[39;00m nonce:\n\u001b[1;32m   2100\u001b[0m         seed \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[0;32m-> 2101\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mstateless_uniform(\n\u001b[1;32m   2102\u001b[0m         shape\u001b[39m=\u001b[39;49mshape,\n\u001b[1;32m   2103\u001b[0m         minval\u001b[39m=\u001b[39;49mminval,\n\u001b[1;32m   2104\u001b[0m         maxval\u001b[39m=\u001b[39;49mmaxval,\n\u001b[1;32m   2105\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   2106\u001b[0m         seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m   2107\u001b[0m     )\n\u001b[1;32m   2108\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\n\u001b[1;32m   2109\u001b[0m     shape\u001b[39m=\u001b[39mshape,\n\u001b[1;32m   2110\u001b[0m     minval\u001b[39m=\u001b[39mminval,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_legacy_seed(),\n\u001b[1;32m   2114\u001b[0m )\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[1348320,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2]"
     ]
    }
   ],
   "source": [
    "# Input shape and number of classes\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = 2  # Two classes: original and manipulated\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Convolutional layers\n",
    "conv1 = Conv2D(32, (3, 3), strides=(2, 2), activation='relu')(input_layer)\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu')(conv1)\n",
    "conv2_pool = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(conv2)\n",
    "conv3 = Conv2D(128, (3, 3), padding='valid', activation='relu')(conv2_pool)\n",
    "# ...\n",
    "\n",
    "# Inception Module (Mixed 0)\n",
    "conv1x1_0 = Conv2D(64, (1, 1), activation='relu')(conv3)\n",
    "\n",
    "conv3x3_0 = Conv2D(96, (1, 1), activation='relu')(conv3)\n",
    "conv3x3_0 = Conv2D(128, (3, 3), padding='same', activation='relu')(conv3x3_0)\n",
    "\n",
    "conv5x5_0 = Conv2D(16, (1, 1), activation='relu')(conv3)\n",
    "conv5x5_0 = Conv2D(32, (5, 5), padding='same', activation='relu')(conv5x5_0)\n",
    "\n",
    "pool_0 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(conv3)\n",
    "pool_0 = Conv2D(32, (1, 1), activation='relu')(pool_0)\n",
    "\n",
    "mixed0 = Concatenate(axis=-1)([conv1x1_0, conv3x3_0, conv5x5_0, pool_0])\n",
    "\n",
    "# Inception Module (Mixed 1)\n",
    "conv1x1_1 = Conv2D(128, (1, 1), actregulritazationivation='relu')(mixed0)\n",
    "\n",
    "conv3x3_1 = Conv2D(128, (1, 1), activation='relu')(mixed0)\n",
    "conv3x3_1 = Conv2D(192, (3, 3), padding='same', activation='relu')(conv3x3_1)\n",
    "\n",
    "conv5x5_1 = Conv2D(32, (1, 1), activation='relu')(mixed0)\n",
    "conv5x5_1 = Conv2D(96, (5, 5), padding='same', activation='relu')(conv5x5_1)\n",
    "\n",
    "pool_1 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(mixed0)\n",
    "pool_1 = Conv2D(64, (1, 1), activation='relu')(pool_1)\n",
    "\n",
    "mixed1 = Concatenate(axis=-1)([conv1x1_1, conv3x3_1, conv5x5_1, pool_1])\n",
    "\n",
    "# ...\n",
    "\n",
    "# Auxiliary Classifier\n",
    "aux_pooling = AveragePooling2D(pool_size=(5, 5), strides=(3, 3))(mixed1)  # Replace mixedX with the appropriate layer\n",
    "aux_conv = Conv2D(128, (1, 1), activation='relu')(aux_pooling)\n",
    "aux_flat = Flatten()(aux_conv)\n",
    "aux_fc1 = Dense(1024, activation='relu')(aux_flat)\n",
    "aux_dropout = Dropout(0.7)(aux_fc1)\n",
    "aux_output = Dense(num_classes, activation='softmax')(aux_dropout)\n",
    "\n",
    "# Fully Connected Layers\n",
    "flatten = Flatten()(mixed1)  # Replace mixedX with the appropriate layer\n",
    "fc1 = Dense(2048, activation='relu')(flatten)\n",
    "dropout = Dropout(0.5)(fc1)\n",
    "output = Dense(num_classes, activation='softmax')(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a32f5a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers for copy-move forgery detection\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x) \n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x) \n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x) \n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72f9b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bae65b48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 299, 299, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 149, 149, 32  864         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 149, 149, 32  96         ['conv2d[0][0]']                 \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 149, 149, 32  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 147, 147, 32  9216        ['activation[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 147, 147, 32  96         ['conv2d_1[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 147, 147, 32  0           ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 147, 147, 64  18432       ['activation_1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 147, 147, 64  192        ['conv2d_2[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 147, 147, 64  0           ['batch_normalization_2[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 73, 73, 64)   0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 73, 73, 80)   5120        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 73, 73, 80)  240         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 73, 73, 80)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 71, 71, 192)  138240      ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 71, 71, 192)  576        ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 71, 71, 192)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 35, 35, 192)  0          ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 35, 35, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 35, 35, 64)  192         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 35, 35, 64)   0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 35, 35, 48)   9216        ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 35, 35, 96)   55296       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 35, 35, 48)  144         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 35, 35, 96)  288         ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 35, 35, 48)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 35, 35, 96)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 35, 35, 192)  0          ['max_pooling2d_1[0][0]']        \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 35, 35, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 35, 35, 64)   76800       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 35, 35, 96)   82944       ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 35, 35, 32)   6144        ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 35, 35, 64)  192         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 35, 35, 64)  192         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 35, 35, 96)  288         ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 35, 35, 32)  96          ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 35, 35, 64)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 35, 35, 64)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 35, 35, 96)   0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 35, 35, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " mixed0 (Concatenate)           (None, 35, 35, 256)  0           ['activation_5[0][0]',           \n",
      "                                                                  'activation_7[0][0]',           \n",
      "                                                                  'activation_10[0][0]',          \n",
      "                                                                  'activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 35, 35, 64)   16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 35, 35, 64)  192         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 35, 35, 64)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 35, 35, 48)   12288       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 35, 35, 96)   55296       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 35, 35, 48)  144         ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 35, 35, 96)  288         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 35, 35, 48)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 35, 35, 96)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 35, 35, 256)  0          ['mixed0[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 35, 35, 64)   16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 35, 35, 64)   76800       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 35, 35, 96)   82944       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 35, 35, 64)   16384       ['average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 35, 35, 64)  192         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 35, 35, 64)  192         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 35, 35, 96)  288         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 35, 35, 64)  192         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 35, 35, 64)   0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 35, 35, 64)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 35, 35, 96)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 35, 35, 64)   0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " mixed1 (Concatenate)           (None, 35, 35, 288)  0           ['activation_12[0][0]',          \n",
      "                                                                  'activation_14[0][0]',          \n",
      "                                                                  'activation_17[0][0]',          \n",
      "                                                                  'activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 35, 35, 64)   18432       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 35, 35, 64)  192         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 35, 35, 64)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 35, 35, 48)   13824       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 35, 35, 96)   55296       ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 35, 35, 48)  144         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 35, 35, 96)  288         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 35, 35, 48)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 35, 35, 96)   0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 35, 35, 288)  0          ['mixed1[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 35, 35, 64)   18432       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 35, 35, 64)   76800       ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 35, 35, 96)   82944       ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 35, 35, 64)   18432       ['average_pooling2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 35, 35, 64)  192         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 35, 35, 64)  192         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 35, 35, 96)  288         ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 35, 35, 64)  192         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 35, 35, 64)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 35, 35, 64)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 35, 35, 96)   0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 35, 35, 64)   0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " mixed2 (Concatenate)           (None, 35, 35, 288)  0           ['activation_19[0][0]',          \n",
      "                                                                  'activation_21[0][0]',          \n",
      "                                                                  'activation_24[0][0]',          \n",
      "                                                                  'activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 35, 35, 64)   18432       ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 35, 35, 64)  192         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 35, 35, 64)   0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 35, 35, 96)   55296       ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 35, 35, 96)  288         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 35, 35, 96)   0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 17, 17, 384)  995328      ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 17, 17, 96)   82944       ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 17, 17, 384)  1152       ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 17, 17, 96)  288         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 17, 17, 384)  0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 17, 17, 96)   0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 17, 17, 288)  0          ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " mixed3 (Concatenate)           (None, 17, 17, 768)  0           ['activation_26[0][0]',          \n",
      "                                                                  'activation_29[0][0]',          \n",
      "                                                                  'max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 17, 17, 128)  98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 17, 17, 128)  384        ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 17, 17, 128)  0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 17, 17, 128)  114688      ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 17, 17, 128)  384        ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 17, 17, 128)  0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 17, 17, 128)  98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 17, 17, 128)  114688      ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 17, 17, 128)  384        ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 17, 17, 128)  384        ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 17, 17, 128)  0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 17, 17, 128)  0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 17, 17, 128)  114688      ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 17, 17, 128)  114688      ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 17, 17, 128)  384        ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 17, 17, 128)  384        ['conv2d_37[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 17, 17, 128)  0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 17, 17, 128)  0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (AveragePo  (None, 17, 17, 768)  0          ['mixed3[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 17, 17, 192)  147456      ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 17, 17, 192)  172032      ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 17, 17, 192)  172032      ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 17, 17, 192)  147456      ['average_pooling2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 17, 17, 192)  576        ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 17, 17, 192)  576        ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 17, 17, 192)  576        ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 17, 17, 192)  576        ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " mixed4 (Concatenate)           (None, 17, 17, 768)  0           ['activation_30[0][0]',          \n",
      "                                                                  'activation_33[0][0]',          \n",
      "                                                                  'activation_38[0][0]',          \n",
      "                                                                  'activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 17, 17, 160)  122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 17, 17, 160)  480        ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 17, 17, 160)  0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 17, 17, 160)  179200      ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 17, 17, 160)  480        ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 17, 17, 160)  0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 17, 17, 160)  122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 17, 17, 160)  179200      ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 17, 17, 160)  480        ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 17, 17, 160)  480        ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 17, 17, 160)  0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 17, 17, 160)  0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 17, 17, 160)  179200      ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 17, 17, 160)  179200      ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 17, 17, 160)  480        ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 17, 17, 160)  480        ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 17, 17, 160)  0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 17, 17, 160)  0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_4 (AveragePo  (None, 17, 17, 768)  0          ['mixed4[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 17, 17, 192)  147456      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 17, 17, 192)  215040      ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 17, 17, 192)  215040      ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 17, 17, 192)  147456      ['average_pooling2d_4[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 17, 17, 192)  576        ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 17, 17, 192)  576        ['conv2d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 17, 17, 192)  576        ['conv2d_48[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 17, 17, 192)  576        ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " mixed5 (Concatenate)           (None, 17, 17, 768)  0           ['activation_40[0][0]',          \n",
      "                                                                  'activation_43[0][0]',          \n",
      "                                                                  'activation_48[0][0]',          \n",
      "                                                                  'activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 17, 17, 160)  122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 17, 17, 160)  480        ['conv2d_54[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 17, 17, 160)  0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 17, 17, 160)  179200      ['activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 17, 17, 160)  480        ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 17, 17, 160)  0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 17, 17, 160)  122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 17, 17, 160)  179200      ['activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 17, 17, 160)  480        ['conv2d_51[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 17, 17, 160)  480        ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 17, 17, 160)  0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 17, 17, 160)  0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 17, 17, 160)  179200      ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 17, 17, 160)  179200      ['activation_56[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 17, 17, 160)  480        ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 17, 17, 160)  480        ['conv2d_57[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 17, 17, 160)  0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 17, 17, 160)  0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_5 (AveragePo  (None, 17, 17, 768)  0          ['mixed5[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 17, 17, 192)  147456      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 17, 17, 192)  215040      ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 17, 17, 192)  215040      ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 17, 17, 192)  147456      ['average_pooling2d_5[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 17, 17, 192)  576        ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 17, 17, 192)  576        ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 17, 17, 192)  576        ['conv2d_58[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 17, 17, 192)  576        ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " mixed6 (Concatenate)           (None, 17, 17, 768)  0           ['activation_50[0][0]',          \n",
      "                                                                  'activation_53[0][0]',          \n",
      "                                                                  'activation_58[0][0]',          \n",
      "                                                                  'activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 17, 17, 192)  147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 17, 17, 192)  576        ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 17, 17, 192)  258048      ['activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 17, 17, 192)  576        ['conv2d_65[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 17, 17, 192)  147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 17, 17, 192)  258048      ['activation_65[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 17, 17, 192)  576        ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 17, 17, 192)  576        ['conv2d_66[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " activation_66 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 17, 17, 192)  258048      ['activation_61[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 17, 17, 192)  258048      ['activation_66[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 17, 17, 192)  576        ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 17, 17, 192)  576        ['conv2d_67[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " activation_67 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_6 (AveragePo  (None, 17, 17, 768)  0          ['mixed6[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 17, 17, 192)  147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 17, 17, 192)  258048      ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 17, 17, 192)  258048      ['activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 17, 17, 192)  147456      ['average_pooling2d_6[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 17, 17, 192)  576        ['conv2d_60[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 17, 17, 192)  576        ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 17, 17, 192)  576        ['conv2d_68[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 17, 17, 192)  576        ['conv2d_69[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " activation_68 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " activation_69 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " mixed7 (Concatenate)           (None, 17, 17, 768)  0           ['activation_60[0][0]',          \n",
      "                                                                  'activation_63[0][0]',          \n",
      "                                                                  'activation_68[0][0]',          \n",
      "                                                                  'activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 17, 17, 192)  147456      ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 17, 17, 192)  576        ['conv2d_72[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_72 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 17, 17, 192)  258048      ['activation_72[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 17, 17, 192)  576        ['conv2d_73[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_73 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 17, 17, 192)  147456      ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 17, 17, 192)  258048      ['activation_73[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 17, 17, 192)  576        ['conv2d_70[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 17, 17, 192)  576        ['conv2d_74[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_70 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " activation_74 (Activation)     (None, 17, 17, 192)  0           ['batch_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 8, 8, 320)    552960      ['activation_70[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 8, 8, 192)    331776      ['activation_74[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 8, 8, 320)   960         ['conv2d_71[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 8, 8, 192)   576         ['conv2d_75[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_71 (Activation)     (None, 8, 8, 320)    0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " activation_75 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 768)   0           ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " mixed8 (Concatenate)           (None, 8, 8, 1280)   0           ['activation_71[0][0]',          \n",
      "                                                                  'activation_75[0][0]',          \n",
      "                                                                  'max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 8, 8, 448)    573440      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 8, 8, 448)   1344        ['conv2d_80[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_80 (Activation)     (None, 8, 8, 448)    0           ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 8, 8, 384)    491520      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 8, 8, 384)    1548288     ['activation_80[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 8, 8, 384)   1152        ['conv2d_77[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 8, 8, 384)   1152        ['conv2d_81[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_77 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " activation_81 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 8, 8, 384)    442368      ['activation_77[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 8, 8, 384)    442368      ['activation_77[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 8, 8, 384)    442368      ['activation_81[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 8, 8, 384)    442368      ['activation_81[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_7 (AveragePo  (None, 8, 8, 1280)  0           ['mixed8[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 8, 8, 320)    409600      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_78 (BatchN  (None, 8, 8, 384)   1152        ['conv2d_78[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_79 (BatchN  (None, 8, 8, 384)   1152        ['conv2d_79[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 8, 8, 384)   1152        ['conv2d_82[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_83 (BatchN  (None, 8, 8, 384)   1152        ['conv2d_83[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 8, 8, 192)    245760      ['average_pooling2d_7[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_76 (BatchN  (None, 8, 8, 320)   960         ['conv2d_76[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_78 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_78[0][0]'] \n",
      "                                                                                                  \n",
      " activation_79 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_79[0][0]'] \n",
      "                                                                                                  \n",
      " activation_82 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " activation_83 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_83[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_84 (BatchN  (None, 8, 8, 192)   576         ['conv2d_84[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_76 (Activation)     (None, 8, 8, 320)    0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " mixed9_0 (Concatenate)         (None, 8, 8, 768)    0           ['activation_78[0][0]',          \n",
      "                                                                  'activation_79[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 8, 8, 768)    0           ['activation_82[0][0]',          \n",
      "                                                                  'activation_83[0][0]']          \n",
      "                                                                                                  \n",
      " activation_84 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_84[0][0]'] \n",
      "                                                                                                  \n",
      " mixed9 (Concatenate)           (None, 8, 8, 2048)   0           ['activation_76[0][0]',          \n",
      "                                                                  'mixed9_0[0][0]',               \n",
      "                                                                  'concatenate[0][0]',            \n",
      "                                                                  'activation_84[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)             (None, 8, 8, 448)    917504      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_89 (BatchN  (None, 8, 8, 448)   1344        ['conv2d_89[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_89 (Activation)     (None, 8, 8, 448)    0           ['batch_normalization_89[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)             (None, 8, 8, 384)    786432      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)             (None, 8, 8, 384)    1548288     ['activation_89[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 8, 8, 384)   1152        ['conv2d_86[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 8, 8, 384)   1152        ['conv2d_90[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_86 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " activation_90 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)             (None, 8, 8, 384)    442368      ['activation_86[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)             (None, 8, 8, 384)    442368      ['activation_86[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)             (None, 8, 8, 384)    442368      ['activation_90[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)             (None, 8, 8, 384)    442368      ['activation_90[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_8 (AveragePo  (None, 8, 8, 2048)  0           ['mixed9[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)             (None, 8, 8, 320)    655360      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 8, 8, 384)   1152        ['conv2d_87[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_88 (BatchN  (None, 8, 8, 384)   1152        ['conv2d_88[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 8, 8, 384)   1152        ['conv2d_91[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 8, 8, 384)   1152        ['conv2d_92[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)             (None, 8, 8, 192)    393216      ['average_pooling2d_8[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 8, 8, 320)   960         ['conv2d_85[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_87 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " activation_88 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_88[0][0]'] \n",
      "                                                                                                  \n",
      " activation_91 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " activation_92 (Activation)     (None, 8, 8, 384)    0           ['batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_93 (BatchN  (None, 8, 8, 192)   576         ['conv2d_93[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_85 (Activation)     (None, 8, 8, 320)    0           ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " mixed9_1 (Concatenate)         (None, 8, 8, 768)    0           ['activation_87[0][0]',          \n",
      "                                                                  'activation_88[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 8, 8, 768)    0           ['activation_91[0][0]',          \n",
      "                                                                  'activation_92[0][0]']          \n",
      "                                                                                                  \n",
      " activation_93 (Activation)     (None, 8, 8, 192)    0           ['batch_normalization_93[0][0]'] \n",
      "                                                                                                  \n",
      " mixed10 (Concatenate)          (None, 8, 8, 2048)   0           ['activation_85[0][0]',          \n",
      "                                                                  'mixed9_1[0][0]',               \n",
      "                                                                  'concatenate_1[0][0]',          \n",
      "                                                                  'activation_93[0][0]']          \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2048)        0           ['mixed10[0][0]']                \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         2098176     ['global_average_pooling2d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            1025        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,901,985\n",
      "Trainable params: 23,867,553\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=BinaryCrossentropy(),\n",
    "              metrics=[AUC(), BinaryAccuracy(), Precision(), Recall(), FalseNegatives(), FalsePositives(), TrueNegatives(), TruePositives()])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "710bd226",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.0000001, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61c9be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/media/maged/New Volume/NU/1-Spring 23/Grad Project 2/Attempt/inception V3 copy move.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cc4bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3400ba2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 21:24:00.036776: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-07-03 21:24:07.528025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-07-03 21:24:10.571984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-07-03 21:24:10.576166: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x32ce3350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-03 21:24:10.576182: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Ti, Compute Capability 8.6\n",
      "2023-07-03 21:24:10.579601: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-07-03 21:24:10.681311: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4196/4196 [==============================] - ETA: 0s - loss: 0.6963 - auc: 0.5096 - binary_accuracy: 0.5118 - precision: 0.5050 - recall: 0.2282 - false_negatives: 12706.0000 - false_positives: 3682.0000 - true_negatives: 13421.0000 - true_positives: 3756.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 21:47:01.265500: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4196/4196 [==============================] - 1545s 361ms/step - loss: 0.6963 - auc: 0.5096 - binary_accuracy: 0.5118 - precision: 0.5050 - recall: 0.2282 - false_negatives: 12706.0000 - false_positives: 3682.0000 - true_negatives: 13421.0000 - true_positives: 3756.0000 - val_loss: 1.0324 - val_auc: 0.5322 - val_binary_accuracy: 0.5084 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_negatives: 2828.0000 - val_false_positives: 0.0000e+00 - val_true_negatives: 2925.0000 - val_true_positives: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "4196/4196 [==============================] - 1229s 293ms/step - loss: 0.6930 - auc: 0.5092 - binary_accuracy: 0.5099 - precision: 0.5013 - recall: 0.1287 - false_negatives: 14344.0000 - false_positives: 2107.0000 - true_negatives: 14996.0000 - true_positives: 2118.0000 - val_loss: 0.6929 - val_auc: 0.5032 - val_binary_accuracy: 0.5084 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_false_negatives: 2828.0000 - val_false_positives: 0.0000e+00 - val_true_negatives: 2925.0000 - val_true_positives: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "4196/4196 [==============================] - 1090s 260ms/step - loss: 0.6931 - auc: 0.5068 - binary_accuracy: 0.5121 - precision: 0.5144 - recall: 0.0932 - false_negatives: 14927.0000 - false_positives: 1449.0000 - true_negatives: 15654.0000 - true_positives: 1535.0000 - val_loss: 0.6929 - val_auc: 0.5136 - val_binary_accuracy: 0.5157 - val_precision: 0.5714 - val_recall: 0.0594 - val_false_negatives: 2660.0000 - val_false_positives: 126.0000 - val_true_negatives: 2799.0000 - val_true_positives: 168.0000 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "4196/4196 [==============================] - 1066s 254ms/step - loss: 0.6926 - auc: 0.5224 - binary_accuracy: 0.5217 - precision: 0.5313 - recall: 0.2098 - false_negatives: 13009.0000 - false_positives: 3046.0000 - true_negatives: 14057.0000 - true_positives: 3453.0000 - val_loss: 1.2954 - val_auc: 0.5170 - val_binary_accuracy: 0.5098 - val_precision: 0.5013 - val_recall: 0.5410 - val_false_negatives: 1298.0000 - val_false_positives: 1522.0000 - val_true_negatives: 1403.0000 - val_true_positives: 1530.0000 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "4196/4196 [==============================] - ETA: 0s - loss: 0.6861 - auc: 0.5672 - binary_accuracy: 0.5510 - precision: 0.5524 - recall: 0.4455 - false_negatives: 9129.0000 - false_positives: 5941.0000 - true_negatives: 11162.0000 - true_positives: 7333.0000\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "4196/4196 [==============================] - 1025s 244ms/step - loss: 0.6861 - auc: 0.5672 - binary_accuracy: 0.5510 - precision: 0.5524 - recall: 0.4455 - false_negatives: 9129.0000 - false_positives: 5941.0000 - true_negatives: 11162.0000 - true_positives: 7333.0000 - val_loss: 0.7004 - val_auc: 0.6031 - val_binary_accuracy: 0.5762 - val_precision: 0.5573 - val_recall: 0.6708 - val_false_negatives: 931.0000 - val_false_positives: 1507.0000 - val_true_negatives: 1418.0000 - val_true_positives: 1897.0000 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "4196/4196 [==============================] - 1039s 248ms/step - loss: 0.6598 - auc: 0.6450 - binary_accuracy: 0.6105 - precision: 0.5943 - recall: 0.6488 - false_negatives: 5782.0000 - false_positives: 7291.0000 - true_negatives: 9812.0000 - true_positives: 10680.0000 - val_loss: 0.6671 - val_auc: 0.6465 - val_binary_accuracy: 0.6172 - val_precision: 0.6046 - val_recall: 0.6397 - val_false_negatives: 1019.0000 - val_false_positives: 1183.0000 - val_true_negatives: 1742.0000 - val_true_positives: 1809.0000 - lr: 5.0000e-04\n",
      "Epoch 7/50\n",
      "4196/4196 [==============================] - 1035s 247ms/step - loss: 0.6466 - auc: 0.6708 - binary_accuracy: 0.6261 - precision: 0.6101 - recall: 0.6584 - false_negatives: 5624.0000 - false_positives: 6925.0000 - true_negatives: 10178.0000 - true_positives: 10838.0000 - val_loss: 0.7990 - val_auc: 0.6540 - val_binary_accuracy: 0.6188 - val_precision: 0.6063 - val_recall: 0.6404 - val_false_negatives: 1017.0000 - val_false_positives: 1176.0000 - val_true_negatives: 1749.0000 - val_true_positives: 1811.0000 - lr: 5.0000e-04\n",
      "Epoch 8/50\n",
      "4196/4196 [==============================] - 1054s 251ms/step - loss: 0.6327 - auc: 0.6947 - binary_accuracy: 0.6453 - precision: 0.6293 - recall: 0.6737 - false_negatives: 5372.0000 - false_positives: 6534.0000 - true_negatives: 10569.0000 - true_positives: 11090.0000 - val_loss: 0.6665 - val_auc: 0.6803 - val_binary_accuracy: 0.6332 - val_precision: 0.6201 - val_recall: 0.6556 - val_false_negatives: 974.0000 - val_false_positives: 1136.0000 - val_true_negatives: 1789.0000 - val_true_positives: 1854.0000 - lr: 5.0000e-04\n",
      "Epoch 9/50\n",
      "4196/4196 [==============================] - 1071s 255ms/step - loss: 0.6143 - auc: 0.7206 - binary_accuracy: 0.6637 - precision: 0.6461 - recall: 0.6948 - false_negatives: 5024.0000 - false_positives: 6265.0000 - true_negatives: 10838.0000 - true_positives: 11438.0000 - val_loss: 0.6425 - val_auc: 0.7056 - val_binary_accuracy: 0.6463 - val_precision: 0.6472 - val_recall: 0.6163 - val_false_negatives: 1085.0000 - val_false_positives: 950.0000 - val_true_negatives: 1975.0000 - val_true_positives: 1743.0000 - lr: 5.0000e-04\n",
      "Epoch 10/50\n",
      "4196/4196 [==============================] - 1079s 257ms/step - loss: 0.6023 - auc: 0.7366 - binary_accuracy: 0.6768 - precision: 0.6604 - recall: 0.7019 - false_negatives: 4908.0000 - false_positives: 5941.0000 - true_negatives: 11162.0000 - true_positives: 11554.0000 - val_loss: 0.6794 - val_auc: 0.6704 - val_binary_accuracy: 0.6192 - val_precision: 0.5838 - val_recall: 0.7843 - val_false_negatives: 610.0000 - val_false_positives: 1581.0000 - val_true_negatives: 1344.0000 - val_true_positives: 2218.0000 - lr: 5.0000e-04\n",
      "Epoch 11/50\n",
      "4196/4196 [==============================] - 1090s 260ms/step - loss: 0.5885 - auc: 0.7527 - binary_accuracy: 0.6881 - precision: 0.6740 - recall: 0.7051 - false_negatives: 4855.0000 - false_positives: 5613.0000 - true_negatives: 11490.0000 - true_positives: 11607.0000 - val_loss: 0.6178 - val_auc: 0.7192 - val_binary_accuracy: 0.6619 - val_precision: 0.6386 - val_recall: 0.7192 - val_false_negatives: 794.0000 - val_false_positives: 1151.0000 - val_true_negatives: 1774.0000 - val_true_positives: 2034.0000 - lr: 5.0000e-04\n",
      "Epoch 12/50\n",
      "4196/4196 [==============================] - 1100s 262ms/step - loss: 0.5571 - auc: 0.7828 - binary_accuracy: 0.7103 - precision: 0.6851 - recall: 0.7574 - false_negatives: 3993.0000 - false_positives: 5732.0000 - true_negatives: 11371.0000 - true_positives: 12469.0000 - val_loss: 0.6012 - val_auc: 0.7302 - val_binary_accuracy: 0.6725 - val_precision: 0.6371 - val_recall: 0.7755 - val_false_negatives: 635.0000 - val_false_positives: 1249.0000 - val_true_negatives: 1676.0000 - val_true_positives: 2193.0000 - lr: 5.0000e-04\n",
      "Epoch 13/50\n",
      "4196/4196 [==============================] - 1099s 262ms/step - loss: 0.5370 - auc: 0.7973 - binary_accuracy: 0.7213 - precision: 0.6897 - recall: 0.7848 - false_negatives: 3543.0000 - false_positives: 5813.0000 - true_negatives: 11290.0000 - true_positives: 12919.0000 - val_loss: 0.5541 - val_auc: 0.7678 - val_binary_accuracy: 0.6975 - val_precision: 0.6713 - val_recall: 0.7539 - val_false_negatives: 696.0000 - val_false_positives: 1044.0000 - val_true_negatives: 1881.0000 - val_true_positives: 2132.0000 - lr: 5.0000e-04\n",
      "Epoch 14/50\n",
      "4196/4196 [==============================] - 1104s 263ms/step - loss: 0.5212 - auc: 0.8082 - binary_accuracy: 0.7313 - precision: 0.6949 - recall: 0.8058 - false_negatives: 3197.0000 - false_positives: 5823.0000 - true_negatives: 11280.0000 - true_positives: 13265.0000 - val_loss: 0.6847 - val_auc: 0.7444 - val_binary_accuracy: 0.6864 - val_precision: 0.6217 - val_recall: 0.9250 - val_false_negatives: 212.0000 - val_false_positives: 1592.0000 - val_true_negatives: 1333.0000 - val_true_positives: 2616.0000 - lr: 5.0000e-04\n",
      "Epoch 15/50\n",
      "4196/4196 [==============================] - 1111s 265ms/step - loss: 0.5030 - auc: 0.8218 - binary_accuracy: 0.7446 - precision: 0.7041 - recall: 0.8267 - false_negatives: 2853.0000 - false_positives: 5720.0000 - true_negatives: 11383.0000 - true_positives: 13609.0000 - val_loss: 0.5579 - val_auc: 0.7727 - val_binary_accuracy: 0.7083 - val_precision: 0.6771 - val_recall: 0.7772 - val_false_negatives: 630.0000 - val_false_positives: 1048.0000 - val_true_negatives: 1877.0000 - val_true_positives: 2198.0000 - lr: 5.0000e-04\n",
      "Epoch 16/50\n",
      "4196/4196 [==============================] - 1111s 265ms/step - loss: 0.4929 - auc: 0.8279 - binary_accuracy: 0.7510 - precision: 0.7102 - recall: 0.8317 - false_negatives: 2771.0000 - false_positives: 5588.0000 - true_negatives: 11515.0000 - true_positives: 13691.0000 - val_loss: 0.5381 - val_auc: 0.7982 - val_binary_accuracy: 0.7108 - val_precision: 0.7265 - val_recall: 0.6602 - val_false_negatives: 961.0000 - val_false_positives: 703.0000 - val_true_negatives: 2222.0000 - val_true_positives: 1867.0000 - lr: 5.0000e-04\n",
      "Epoch 17/50\n",
      "4196/4196 [==============================] - 1106s 264ms/step - loss: 0.4860 - auc: 0.8335 - binary_accuracy: 0.7565 - precision: 0.7167 - recall: 0.8326 - false_negatives: 2756.0000 - false_positives: 5418.0000 - true_negatives: 11685.0000 - true_positives: 13706.0000 - val_loss: 0.5629 - val_auc: 0.7883 - val_binary_accuracy: 0.7257 - val_precision: 0.6636 - val_recall: 0.8964 - val_false_negatives: 293.0000 - val_false_positives: 1285.0000 - val_true_negatives: 1640.0000 - val_true_positives: 2535.0000 - lr: 5.0000e-04\n",
      "Epoch 18/50\n",
      "4196/4196 [==============================] - 1112s 265ms/step - loss: 0.4761 - auc: 0.8402 - binary_accuracy: 0.7624 - precision: 0.7212 - recall: 0.8404 - false_negatives: 2627.0000 - false_positives: 5347.0000 - true_negatives: 11756.0000 - true_positives: 13835.0000 - val_loss: 0.5993 - val_auc: 0.7796 - val_binary_accuracy: 0.7177 - val_precision: 0.6624 - val_recall: 0.8681 - val_false_negatives: 373.0000 - val_false_positives: 1251.0000 - val_true_negatives: 1674.0000 - val_true_positives: 2455.0000 - lr: 5.0000e-04\n",
      "Epoch 19/50\n",
      "4196/4196 [==============================] - 1117s 266ms/step - loss: 0.4699 - auc: 0.8445 - binary_accuracy: 0.7675 - precision: 0.7255 - recall: 0.8459 - false_negatives: 2536.0000 - false_positives: 5269.0000 - true_negatives: 11834.0000 - true_positives: 13926.0000 - val_loss: 0.5360 - val_auc: 0.7968 - val_binary_accuracy: 0.7318 - val_precision: 0.6745 - val_recall: 0.8780 - val_false_negatives: 345.0000 - val_false_positives: 1198.0000 - val_true_negatives: 1727.0000 - val_true_positives: 2483.0000 - lr: 5.0000e-04\n",
      "Epoch 20/50\n",
      "4196/4196 [==============================] - 1122s 267ms/step - loss: 0.4656 - auc: 0.8479 - binary_accuracy: 0.7687 - precision: 0.7286 - recall: 0.8421 - false_negatives: 2599.0000 - false_positives: 5163.0000 - true_negatives: 11940.0000 - true_positives: 13863.0000 - val_loss: 0.5837 - val_auc: 0.8019 - val_binary_accuracy: 0.7040 - val_precision: 0.7327 - val_recall: 0.6262 - val_false_negatives: 1057.0000 - val_false_positives: 646.0000 - val_true_negatives: 2279.0000 - val_true_positives: 1771.0000 - lr: 5.0000e-04\n",
      "Epoch 21/50\n",
      "4196/4196 [==============================] - 1128s 269ms/step - loss: 0.4606 - auc: 0.8518 - binary_accuracy: 0.7740 - precision: 0.7343 - recall: 0.8450 - false_negatives: 2552.0000 - false_positives: 5034.0000 - true_negatives: 12069.0000 - true_positives: 13910.0000 - val_loss: 0.5281 - val_auc: 0.8056 - val_binary_accuracy: 0.7316 - val_precision: 0.6779 - val_recall: 0.8649 - val_false_negatives: 382.0000 - val_false_positives: 1162.0000 - val_true_negatives: 1763.0000 - val_true_positives: 2446.0000 - lr: 5.0000e-04\n",
      "Epoch 22/50\n",
      "4196/4196 [==============================] - 1128s 269ms/step - loss: 0.4592 - auc: 0.8519 - binary_accuracy: 0.7727 - precision: 0.7315 - recall: 0.8475 - false_negatives: 2511.0000 - false_positives: 5120.0000 - true_negatives: 11983.0000 - true_positives: 13951.0000 - val_loss: 0.5379 - val_auc: 0.8114 - val_binary_accuracy: 0.7407 - val_precision: 0.6751 - val_recall: 0.9105 - val_false_negatives: 253.0000 - val_false_positives: 1239.0000 - val_true_negatives: 1686.0000 - val_true_positives: 2575.0000 - lr: 5.0000e-04\n",
      "Epoch 23/50\n",
      "4196/4196 [==============================] - 1116s 266ms/step - loss: 0.4519 - auc: 0.8571 - binary_accuracy: 0.7746 - precision: 0.7311 - recall: 0.8548 - false_negatives: 2391.0000 - false_positives: 5175.0000 - true_negatives: 11928.0000 - true_positives: 14071.0000 - val_loss: 0.5300 - val_auc: 0.8016 - val_binary_accuracy: 0.7241 - val_precision: 0.6764 - val_recall: 0.8412 - val_false_negatives: 449.0000 - val_false_positives: 1138.0000 - val_true_negatives: 1787.0000 - val_true_positives: 2379.0000 - lr: 5.0000e-04\n",
      "Epoch 24/50\n",
      "4196/4196 [==============================] - 1129s 269ms/step - loss: 0.4484 - auc: 0.8595 - binary_accuracy: 0.7796 - precision: 0.7363 - recall: 0.8579 - false_negatives: 2339.0000 - false_positives: 5059.0000 - true_negatives: 12044.0000 - true_positives: 14123.0000 - val_loss: 0.4973 - val_auc: 0.8198 - val_binary_accuracy: 0.7480 - val_precision: 0.7045 - val_recall: 0.8395 - val_false_negatives: 454.0000 - val_false_positives: 996.0000 - val_true_negatives: 1929.0000 - val_true_positives: 2374.0000 - lr: 5.0000e-04\n",
      "Epoch 25/50\n",
      "4196/4196 [==============================] - 1120s 267ms/step - loss: 0.4447 - auc: 0.8625 - binary_accuracy: 0.7828 - precision: 0.7436 - recall: 0.8505 - false_negatives: 2461.0000 - false_positives: 4828.0000 - true_negatives: 12275.0000 - true_positives: 14001.0000 - val_loss: 0.7321 - val_auc: 0.7691 - val_binary_accuracy: 0.7064 - val_precision: 0.6393 - val_recall: 0.9240 - val_false_negatives: 215.0000 - val_false_positives: 1474.0000 - val_true_negatives: 1451.0000 - val_true_positives: 2613.0000 - lr: 5.0000e-04\n",
      "Epoch 26/50\n",
      "4196/4196 [==============================] - 1132s 270ms/step - loss: 0.4422 - auc: 0.8656 - binary_accuracy: 0.7854 - precision: 0.7459 - recall: 0.8531 - false_negatives: 2419.0000 - false_positives: 4783.0000 - true_negatives: 12320.0000 - true_positives: 14043.0000 - val_loss: 0.4890 - val_auc: 0.8251 - val_binary_accuracy: 0.7387 - val_precision: 0.6956 - val_recall: 0.8331 - val_false_negatives: 472.0000 - val_false_positives: 1031.0000 - val_true_negatives: 1894.0000 - val_true_positives: 2356.0000 - lr: 5.0000e-04\n",
      "Epoch 27/50\n",
      "4196/4196 [==============================] - 1133s 270ms/step - loss: 0.4416 - auc: 0.8641 - binary_accuracy: 0.7848 - precision: 0.7405 - recall: 0.8641 - false_negatives: 2237.0000 - false_positives: 4985.0000 - true_negatives: 12118.0000 - true_positives: 14225.0000 - val_loss: 0.4987 - val_auc: 0.8237 - val_binary_accuracy: 0.7353 - val_precision: 0.6853 - val_recall: 0.8533 - val_false_negatives: 415.0000 - val_false_positives: 1108.0000 - val_true_negatives: 1817.0000 - val_true_positives: 2413.0000 - lr: 5.0000e-04\n",
      "Epoch 28/50\n",
      "4196/4196 [==============================] - 1125s 268ms/step - loss: 0.4401 - auc: 0.8653 - binary_accuracy: 0.7841 - precision: 0.7397 - recall: 0.8636 - false_negatives: 2245.0000 - false_positives: 5003.0000 - true_negatives: 12100.0000 - true_positives: 14217.0000 - val_loss: 0.5828 - val_auc: 0.8093 - val_binary_accuracy: 0.7301 - val_precision: 0.6694 - val_recall: 0.8907 - val_false_negatives: 309.0000 - val_false_positives: 1244.0000 - val_true_negatives: 1681.0000 - val_true_positives: 2519.0000 - lr: 5.0000e-04\n",
      "Epoch 29/50\n",
      "4196/4196 [==============================] - ETA: 0s - loss: 0.4324 - auc: 0.8700 - binary_accuracy: 0.7895 - precision: 0.7468 - recall: 0.8637 - false_negatives: 2243.0000 - false_positives: 4822.0000 - true_negatives: 12281.0000 - true_positives: 14219.0000\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "4196/4196 [==============================] - 1136s 271ms/step - loss: 0.4324 - auc: 0.8700 - binary_accuracy: 0.7895 - precision: 0.7468 - recall: 0.8637 - false_negatives: 2243.0000 - false_positives: 4822.0000 - true_negatives: 12281.0000 - true_positives: 14219.0000 - val_loss: 0.5214 - val_auc: 0.8248 - val_binary_accuracy: 0.7394 - val_precision: 0.6806 - val_recall: 0.8854 - val_false_negatives: 324.0000 - val_false_positives: 1175.0000 - val_true_negatives: 1750.0000 - val_true_positives: 2504.0000 - lr: 5.0000e-04\n",
      "Epoch 30/50\n",
      "4196/4196 [==============================] - 1142s 272ms/step - loss: 0.4246 - auc: 0.8738 - binary_accuracy: 0.7933 - precision: 0.7490 - recall: 0.8701 - false_negatives: 2139.0000 - false_positives: 4799.0000 - true_negatives: 12304.0000 - true_positives: 14323.0000 - val_loss: 0.5020 - val_auc: 0.8387 - val_binary_accuracy: 0.7495 - val_precision: 0.6875 - val_recall: 0.8992 - val_false_negatives: 285.0000 - val_false_positives: 1156.0000 - val_true_negatives: 1769.0000 - val_true_positives: 2543.0000 - lr: 2.5000e-04\n",
      "Epoch 31/50\n",
      "4196/4196 [==============================] - 1142s 272ms/step - loss: 0.4231 - auc: 0.8760 - binary_accuracy: 0.7927 - precision: 0.7506 - recall: 0.8646 - false_negatives: 2229.0000 - false_positives: 4729.0000 - true_negatives: 12374.0000 - true_positives: 14233.0000 - val_loss: 0.4904 - val_auc: 0.8310 - val_binary_accuracy: 0.7511 - val_precision: 0.6924 - val_recall: 0.8883 - val_false_negatives: 316.0000 - val_false_positives: 1116.0000 - val_true_negatives: 1809.0000 - val_true_positives: 2512.0000 - lr: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_batches,\n",
    "    verbose=1,\n",
    "    validation_data=valid_batches,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping,reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "074a0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('inception V3 copy move_GPT7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af63d584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 95). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Final Model.json\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Final Model.json\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('Final Model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c30e9432",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('D:\\\\Uni\\\\Grad\\\\2\\\\Attempt\\\\My Final Model Weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ee56f05",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'Final Model.json\\\\assets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-22286f0ae763>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# load json and create model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mjson_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Final Model.json\\\\assets'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mloaded_model_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'Final Model.json\\\\assets'"
     ]
    }
   ],
   "source": [
    "Customow.keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('Final Model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ce23a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data generator\n",
    "class CustomImageDataGenerator(tf.keras.preprocessing.image.ImageDataGenerator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def flow_from_directory(self, *args, **kwargs):\n",
    "        iterator = super().flow_from_directory(*args, **kwargs)\n",
    "        while True:\n",
    "            try:\n",
    "                data = next(iterator)\n",
    "                yield data\n",
    "            except (OSError, tf.errors.InvalidArgumentError):\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9986673",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('/media/maged/New Volume/NU/1-Spring 23/Grad Project 2/Attempt/inception V3 copy move_4.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d94192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data_gn = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62c50ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4446 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_batches = img_data_gn.flow_from_directory(test_path,\n",
    "                                               target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                               class_mode='binary',\n",
    "                                               classes=['authentic', 'tampered'],\n",
    "                                               batch_size=8,\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b3bb49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_count = sum([len(files) for _, _, files in os.walk(test_path)])\n",
    "test_steps = math.ceil(test_image_count / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1fa3659",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(learning_rate=0.001), loss=BinaryCrossentropy(),\n",
    "                 metrics=[AUC(), BinaryAccuracy(), Precision(), Recall(), FalseNegatives(),\n",
    "                          FalsePositives(), TrueNegatives(), TruePositives() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2d9ef5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 09:37:11.688444: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 19/556 [>.............................] - ETA: 8:41 - loss: 0.7643 - auc_3: 0.7127 - binary_accuracy: 0.6392 - precision_3: 0.6057 - recall_3: 0.7754 - false_negatives_3: 494.0000 - false_positives_3: 1110.0000 - true_negatives_3: 1137.0000 - true_positives_3: 1705.0000WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 556 batches). You may need to use the repeat() function when building your dataset.\n",
      "556/556 [==============================] - 20s 32ms/step - loss: 0.7643 - auc_3: 0.7127 - binary_accuracy: 0.6392 - precision_3: 0.6057 - recall_3: 0.7754 - false_negatives_3: 494.0000 - false_positives_3: 1110.0000 - true_negatives_3: 1137.0000 - true_positives_3: 1705.0000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_loss, test_accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(test_batches, steps\u001b[39m=\u001b[39mtest_steps)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_batches, steps=test_steps)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d8a7d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 09:41:09.843105: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556/556 [==============================] - 20s 33ms/step\n",
      "Accuracy: 0.5053981106612686\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test generator\n",
    "predictions = model.predict(test_batches)\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get the true labels from the generator\n",
    "true_labels = test_batches.classes\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predicted_labels == true_labels)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9eaa55f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "test_directory = \"/media/maged/New Volume/NU/1-Spring 23/Grad Project 2/datasets again/Defacto + others/test\" # Replace with the path to your test directory\n",
    "\n",
    "test_labels = []\n",
    "\n",
    "authentic_folder = os.path.join(test_directory, \"authentic\")\n",
    "tampered_folder = os.path.join(test_directory, \"tampered\")\n",
    "\n",
    "authentic_files = os.listdir(authentic_folder)\n",
    "tampered_files = os.listdir(tampered_folder)\n",
    "\n",
    "for _ in range(len(authentic_files)):\n",
    "    test_labels.append(0)  # 0 represents the label for authentic images\n",
    "\n",
    "for _ in range(len(tampered_files)):\n",
    "    test_labels.append(1)  # 1 represents the label for tampered images\n",
    "\n",
    "print(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a7da393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 09:33:21.784055: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 19s 982ms/step\n",
      "Accuracy: 0.5053981106612686\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test_batches)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "accuracy = np.mean(y_pred_classes == test_labels)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('/home/maged/ResNet50_3.h5', 'r') as f:\n",
    "    print(list(f.keys()))  # Print the keys of groups and datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5fea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = newModel.to_json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dae41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CNNImageSplicingDetectorModelResNet50_4k.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb98bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for steps in range(len(test_batches)):\n",
    "    test_images, test_labels = next(test_batches)\n",
    "    (newModel.evaluate(test_images, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c9dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#serialize weights to HDF5\n",
    "newModel.save_weights(\"ResNet50.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e4185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98241ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "um_test_samples = len(test_batches.labels)\n",
    "\n",
    "# Generate predictions for the test data using the model\n",
    "y_pred = model.predict(test_batches)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Get the true labels from the test data generator\n",
    "y_true = test_batches.labels\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "class_labels = test_batches.class_indices.keys()\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d',\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
